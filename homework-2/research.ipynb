{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(tokens=2, sample=['comedy', 'action'])\n"
     ]
    }
   ],
   "source": [
    "from src.encoder import Encoder\n",
    "\n",
    "labeler = Encoder(['action', 'comedy'])\n",
    "print(labeler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(tokens=7, sample=['shoot', 'fun', 'fly', 'love', 'fast', 'couple', 'furious'])\n"
     ]
    }
   ],
   "source": [
    "vocabulary = Encoder.open('movie-review-small/aclImdb/imdb.vocab')\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus(documents=5, tokens=7, words=20))\n",
      "{'fun': 1, 'couple': 1, 'love': 2} comedy\n",
      "{'couple': 1, 'fly': 1, 'fast': 1, 'fun': 2} comedy\n",
      "{'fast': 1, 'furious': 1, 'shoot': 1} action\n",
      "{'fly': 1, 'fast': 1, 'shoot': 1, 'love': 1} action\n",
      "{'furious': 1, 'shoot': 2, 'fun': 1} action\n"
     ]
    }
   ],
   "source": [
    "from src.structures.corpus import Corpus\n",
    "from src.structures.document import Document\n",
    "\n",
    "train_corpus = Corpus.open('movie-review-small/aclImdb/train/**/*.txt', vocabulary=vocabulary, verbose=True)\n",
    "print(train_corpus)\n",
    "for document, label in zip(train_corpus, train_corpus.labels()):\n",
    "    print(document, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie-review-small/aclImdb/train/comedy/i.txt {'fun': 1, 'couple': 1, 'love': 2}\n",
      "movie-review-small/aclImdb/train/comedy/iii.txt {'couple': 1, 'fly': 1, 'fast': 1, 'fun': 2}\n",
      "movie-review-small/aclImdb/train/action/ii.txt {'fast': 1, 'furious': 1, 'shoot': 1}\n",
      "movie-review-small/aclImdb/train/action/v.txt {'fly': 1, 'fast': 1, 'shoot': 1, 'love': 1}\n",
      "movie-review-small/aclImdb/train/action/iv.txt {'furious': 1, 'shoot': 2, 'fun': 1}\n"
     ]
    }
   ],
   "source": [
    "for document in train_corpus.documents():\n",
    "    print(document.source, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the corpus\n",
    "\n",
    "frequencies+label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus.write('movie-review-small/aclImdb/train.NB', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c(C)   | value\n",
      "-------+------\n",
      "comedy |     2\n",
      "action |     3\n",
      "p(C)   | value\n",
      "-------+------\n",
      "comedy |  None\n",
      "action |  None\n",
      "c(t,C) | shoot | fun | fly | love | fast | couple | furious\n",
      "-------+-------+-----+-----+------+------+--------+--------\n",
      "comedy |     0 |   3 |   1 |    2 |    1 |      2 |       0\n",
      "action |     4 |   1 |   1 |    1 |    2 |      0 |       2\n",
      "P(t|C) | shoot |  fun |  fly | love | fast | couple | furious\n",
      "-------+-------+------+------+------+------+--------+--------\n",
      "comedy |  None | None | None | None | None |   None |    None\n",
      "action |  None | None | None | None | None |   None |    None\n"
     ]
    }
   ],
   "source": [
    "from src.model import Model\n",
    "\n",
    "model = Model(vocabulary, labeler)\n",
    "model.fit(train_corpus, train_corpus.labels())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and save the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fast': 1, 'couple': 1, 'shoot': 1, 'fly': 1} action\n"
     ]
    }
   ],
   "source": [
    "test_corpus = Corpus.open('movie-review-small/aclImdb/test/**/*.txt', vocabulary=vocabulary, verbose=True)\n",
    "test_corpus.write('movie-review-small/aclImdb/test.NB', verbose=True)\n",
    "\n",
    "for document, label in zip(test_corpus, test_corpus.labels()):\n",
    "    print(document, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(C|d) |                  value\n",
      "-------+-----------------------\n",
      "comedy |  7.324218750000001e-05\n",
      "action | 0.00017146776406035664\n",
      "action movie-review-small/aclImdb/test/action/0.txt\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(Document('fast,couple,shoot,fly'), debug=True)\n",
    "print(labeler.decode(prediction), document.source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(C|d) |                  value\n",
      "-------+-----------------------\n",
      "comedy |  7.324218750000001e-05\n",
      "action | 0.00017146776406035664\n",
      "predict(Corpus(documents=1, tokens=4, words=4))) [1] ['action']\n",
      "accuracy 1.0\n",
      "true\\predicted | comedy | action\n",
      "---------------+--------+-------\n",
      "        comedy |      0 |      0\n",
      "        action |      0 |      1\n"
     ]
    }
   ],
   "source": [
    "from src.metrics import Metrics\n",
    "\n",
    "predictions = model.predict(test_corpus, verbose=True, debug=True)\n",
    "print(f'predict({test_corpus})', predictions, labeler.decode(predictions))\n",
    "score = Metrics.score(test_corpus.labels(), labeler.decode(predictions), labeler)\n",
    "print('accuracy', score['accuracy'])\n",
    "print(score['confusion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c(C)   | value\n",
      "-------+------\n",
      "action |     3\n",
      "comedy |     2\n",
      "p(C)   | value\n",
      "-------+------\n",
      "action |   0.6\n",
      "comedy |   0.4\n",
      "c(t,C) | couple | love | fast | shoot | furious | fly | fun\n",
      "-------+--------+------+------+-------+---------+-----+----\n",
      "action |      0 |    1 |    2 |     4 |       2 |   1 |   1\n",
      "comedy |      2 |    2 |    1 |     0 |       0 |   1 |   3\n",
      "P(t|C) |              couple | love |                fast |              shoot | furious |                fly |  fun\n",
      "-------+---------------------+------+---------------------+--------------------+---------+--------------------+-----\n",
      "action | 0.05555555555555555 | None | 0.16666666666666666 | 0.2777777777777778 |    None | 0.1111111111111111 | None\n",
      "comedy |              0.1875 | None |               0.125 |             0.0625 |    None |              0.125 | None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import text_helpers\n",
    "\n",
    "acronyms, smileys, positive_words, negative_words, negations, stopwords = text_helpers(labeler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a raw review from the corpus\n",
    "\n",
    "```\n",
    "Airwolf The Movie, A variation on the original 2 part pilot, Yet the movie although shorter, does contain extra footage Unseen in the 2 hour pilot The pilot is much more of a pilot than the movie Where as a pilot movie is normally the same (2 parter combined) But the movie is actually a different edit with extras here and cuts there.<br /><br />Worth a look, even if you have the season 1 DVD set, I'd still pick up a copy of the \"movie\" It's still in some shops like virgin, Woolworths and the likes of mixed media stores, although it generally needs ordering, But it saves needing to buy online (as many of us still don't do or trust online shopping) but if you look around airwolfs in stores<br /><br />Airwolf was truly 1 of the 80's most under rated shows.<br /><br />A full size Airwolf is currently being re-built for a Helicopter Museum :) Info and work in progress pictures are over at http://Airwolf.org Also with Airwolf Mods for Flashpoint and Flight Sim Games It seams she's finally here to stay :)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airwolf': 4, 'movie,': 1, 'a': 8, 'variation': 1, 'original': 1, 'part': 1, 'pilot,': 1, 'movie': 4, 'shorter,': 1, 'extra': 1, 'footage': 1, 'unseen': 1, 'hour': 1, 'pilot': 4, '(2': 1, 'parter': 1, 'combined)': 1, 'edit': 1, 'extras': 1, 'cuts': 1, 'there.': 1, 'comedy': 8, 'look,': 1, 'season': 1, '1': 2, 'dvd': 1, 'set,': 1, 'pick': 1, 'copy': 1, '\"movie\"': 1, 'shops': 1, 'virgin,': 1, 'woolworths': 1, 'mixed': 1, 'media': 1, 'stores,': 1, 'generally': 1, 'ordering,': 1, 'saves': 1, 'needing': 1, 'buy': 1, 'online': 2, '(as': 1, '||not||': 1, 'shopping)': 1, 'airwolfs': 1, 'stores': 1, \"80's\": 1, 'rated': 1, 'shows.': 1, 'full': 1, 'size': 1, 're-built': 1, 'helicopter': 1, 'museum': 1, 'info': 1, 'pictures': 1, '||url||': 1, 'mods': 1, 'flashpoint': 1, 'flight': 1, 'sim': 1, 'games': 1, 'seams': 1, 'finally': 1, 'stay': 1}\n"
     ]
    }
   ],
   "source": [
    "from src.structures.bag_of_words import BagOfWords\n",
    "\n",
    "bow = BagOfWords.open(\n",
    "    'movie-review-HW2/aclImdb/train/pos/6770_10.txt',\n",
    "    expansions=acronyms,\n",
    "    replacements={ **smileys, **positive_words, **negative_words, **negations},\n",
    "    ignored=stopwords,\n",
    "    verbose=False)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and saving training data\n",
    "\n",
    "This may take a while, depending on the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(tokens=89527, sample=['virology', 'hi-jacking', 'holidays', 'sachs', 'loring', 'solarisation'])\n",
      "1000 documents loaded\n",
      "2000 documents loaded\n",
      "3000 documents loaded\n",
      "4000 documents loaded\n",
      "5000 documents loaded\n",
      "6000 documents loaded\n",
      "7000 documents loaded\n",
      "8000 documents loaded\n",
      "9000 documents loaded\n",
      "10000 documents loaded\n",
      "11000 documents loaded\n",
      "12000 documents loaded\n",
      "13000 documents loaded\n",
      "14000 documents loaded\n",
      "15000 documents loaded\n",
      "16000 documents loaded\n",
      "17000 documents loaded\n",
      "18000 documents loaded\n",
      "19000 documents loaded\n",
      "20000 documents loaded\n",
      "21000 documents loaded\n",
      "22000 documents loaded\n",
      "23000 documents loaded\n",
      "24000 documents loaded\n",
      "Corpus(documents=25000, tokens=87884, words=5871238))\n",
      "1000 documents written\n",
      "2000 documents written\n",
      "3000 documents written\n",
      "4000 documents written\n",
      "5000 documents written\n",
      "6000 documents written\n",
      "7000 documents written\n",
      "8000 documents written\n",
      "9000 documents written\n",
      "10000 documents written\n",
      "11000 documents written\n",
      "12000 documents written\n",
      "13000 documents written\n",
      "14000 documents written\n",
      "15000 documents written\n",
      "16000 documents written\n",
      "17000 documents written\n",
      "18000 documents written\n",
      "19000 documents written\n",
      "20000 documents written\n",
      "21000 documents written\n",
      "22000 documents written\n",
      "23000 documents written\n",
      "24000 documents written\n",
      "1000 documents loaded\n",
      "2000 documents loaded\n",
      "3000 documents loaded\n",
      "4000 documents loaded\n",
      "5000 documents loaded\n",
      "6000 documents loaded\n",
      "7000 documents loaded\n",
      "8000 documents loaded\n",
      "9000 documents loaded\n",
      "10000 documents loaded\n",
      "11000 documents loaded\n",
      "12000 documents loaded\n",
      "13000 documents loaded\n",
      "14000 documents loaded\n",
      "15000 documents loaded\n",
      "16000 documents loaded\n",
      "17000 documents loaded\n",
      "18000 documents loaded\n",
      "19000 documents loaded\n",
      "20000 documents loaded\n",
      "21000 documents loaded\n",
      "22000 documents loaded\n",
      "23000 documents loaded\n",
      "24000 documents loaded\n",
      "Corpus(documents=25000, tokens=77976, words=5750635))\n",
      "1000 documents written\n",
      "2000 documents written\n",
      "3000 documents written\n",
      "4000 documents written\n",
      "5000 documents written\n",
      "6000 documents written\n",
      "7000 documents written\n",
      "8000 documents written\n",
      "9000 documents written\n",
      "10000 documents written\n",
      "11000 documents written\n",
      "12000 documents written\n",
      "13000 documents written\n",
      "14000 documents written\n",
      "15000 documents written\n",
      "16000 documents written\n",
      "17000 documents written\n",
      "18000 documents written\n",
      "19000 documents written\n",
      "20000 documents written\n",
      "21000 documents written\n",
      "22000 documents written\n",
      "23000 documents written\n",
      "24000 documents written\n"
     ]
    }
   ],
   "source": [
    "vocabulary = Encoder.open('movie-review-HW2/aclImdb/imdb.vocab')\n",
    "print(vocabulary)\n",
    "\n",
    "train_corpus = Corpus.open('movie-review-HW2/aclImdb/train/**/*.txt',\n",
    "    vocabulary=vocabulary,\n",
    "    verbose=True\n",
    ")\n",
    "print(train_corpus)\n",
    "train_corpus.write('movie-review-HW2/aclImdb/train-BOW.NB', verbose=True)\n",
    "\n",
    "test_corpus = Corpus.open('movie-review-HW2/aclImdb/test/**/*.txt',\n",
    "    vocabulary=vocabulary,\n",
    "    verbose=True\n",
    ")\n",
    "print(test_corpus)\n",
    "test_corpus.write('movie-review-HW2/aclImdb/test-BOW.NB', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the saved frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie-review-HW2/aclImdb/train/neg/1821_4.txt'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(tokens=2, sample=['neg', 'pos'])\n",
      "Encoder(tokens=89527, sample=['virology', 'hi-jacking', 'holidays', 'sachs', 'loring', 'solarisation'])\n",
      "1000 documents loaded\n",
      "2000 documents loaded\n",
      "3000 documents loaded\n",
      "4000 documents loaded\n",
      "5000 documents loaded\n",
      "6000 documents loaded\n",
      "7000 documents loaded\n",
      "8000 documents loaded\n",
      "9000 documents loaded\n",
      "10000 documents loaded\n",
      "11000 documents loaded\n",
      "12000 documents loaded\n",
      "13000 documents loaded\n",
      "14000 documents loaded\n",
      "15000 documents loaded\n",
      "16000 documents loaded\n",
      "17000 documents loaded\n",
      "18000 documents loaded\n",
      "19000 documents loaded\n",
      "20000 documents loaded\n",
      "21000 documents loaded\n",
      "22000 documents loaded\n",
      "23000 documents loaded\n",
      "24000 documents loaded\n",
      "Corpus(documents=25000, tokens=87884, words=5871238))\n",
      "1000 documents loaded\n",
      "2000 documents loaded\n",
      "3000 documents loaded\n",
      "4000 documents loaded\n",
      "5000 documents loaded\n",
      "6000 documents loaded\n",
      "7000 documents loaded\n",
      "8000 documents loaded\n",
      "9000 documents loaded\n",
      "10000 documents loaded\n",
      "11000 documents loaded\n",
      "12000 documents loaded\n",
      "13000 documents loaded\n",
      "14000 documents loaded\n",
      "15000 documents loaded\n",
      "16000 documents loaded\n",
      "17000 documents loaded\n",
      "18000 documents loaded\n",
      "19000 documents loaded\n",
      "20000 documents loaded\n",
      "21000 documents loaded\n",
      "22000 documents loaded\n",
      "23000 documents loaded\n",
      "24000 documents loaded\n",
      "Corpus(documents=25000, tokens=77976, words=5750635))\n"
     ]
    }
   ],
   "source": [
    "labeler = Encoder(['pos', 'neg'])\n",
    "print(labeler)\n",
    "vocabulary = Encoder.open('movie-review-HW2/aclImdb/imdb.vocab')\n",
    "print(vocabulary)\n",
    "\n",
    "train_corpus = Corpus.open('movie-review-HW2/aclImdb/train-BOW.NB', frequencies=True, verbose=True)\n",
    "print(train_corpus)\n",
    "test_corpus = Corpus.open('movie-review-HW2/aclImdb/test-BOW.NB', frequencies=True, verbose=True) \n",
    "print(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 documents fitted\n",
      "2000 documents fitted\n",
      "3000 documents fitted\n",
      "4000 documents fitted\n",
      "5000 documents fitted\n",
      "6000 documents fitted\n",
      "7000 documents fitted\n",
      "8000 documents fitted\n",
      "9000 documents fitted\n",
      "10000 documents fitted\n",
      "11000 documents fitted\n",
      "12000 documents fitted\n",
      "13000 documents fitted\n",
      "14000 documents fitted\n",
      "15000 documents fitted\n",
      "16000 documents fitted\n",
      "17000 documents fitted\n",
      "18000 documents fitted\n",
      "19000 documents fitted\n",
      "20000 documents fitted\n",
      "21000 documents fitted\n",
      "22000 documents fitted\n",
      "23000 documents fitted\n",
      "24000 documents fitted\n",
      "Model(vocabulary=Encoder(tokens=89527, sample=['virology', 'hi-jacking', 'holidays', 'sachs', 'loring', 'solarisation']), labeler=Encoder(tokens=2, sample=['neg', 'pos']))\n"
     ]
    }
   ],
   "source": [
    "model = Model(vocabulary, labeler, log=True)\n",
    "model.fit(train_corpus, train_corpus.labels(), verbose=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.81464, 'confusion': [[11041, 1459], [3175, 9325]]}\n"
     ]
    }
   ],
   "source": [
    "from src.metrics import Metrics\n",
    "\n",
    "predictions = model.predict(test_corpus)\n",
    "score = Metrics.score(test_corpus.labels(), labeler.decode(predictions), labeler)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\\predicted |  pos |   neg\n",
      "---------------+------+------\n",
      "           pos | 9343 |  3157\n",
      "           neg | 1531 | 10969\n"
     ]
    }
   ],
   "source": [
    "print(score['confusion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = [document.source for document, y_true, y_pred in zip(test_corpus, test_corpus.labels(), labeler.decode(predictions)) if y_true == 'pos' and y_pred == 'neg' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie-review-HW2/aclImdb/test/pos/3205_9.txt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('movie-review-HW2/aclImdb/test/neg/12179_2.txt').read()[0:200].replace('joke', '')\n",
    "document = Document('staff', labeler, vocabulary)\n",
    "document.label = 'pos'\n",
    "model.predict(document)\n",
    "model.labeler.decode(model.predict(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie-review-HW2/aclImdb/test/pos/2823_10.txt'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = [ (document.label, document.source) for document, y_true, y_pred in zip(test_corpus, test_corpus.labels(), labeler.decode(predictions)) if y_true == 'neg' and y_pred == 'pos' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg', 'movie-review-HW2/aclImdb/test/neg/1821_4.txt')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg movie-review-HW2/aclImdb/test/neg/240_4.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There must be an error. This movie belongs with \"Plan 9\", and a lot others as a quite entertaining, silly diversion. You\\'ll never accept you like it, yet you will watch it whenever it comes out on TV. It\\'s as simple as that.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true, filename = false_positives[151]\n",
    "print(y_true, filename)\n",
    "open(filename).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(tokens=89527, sample=['virology', 'hi-jacking', 'holidays', 'sachs', 'loring', 'solarisation'])\n",
      "1000 documents loaded\n",
      "2000 documents loaded\n",
      "3000 documents loaded\n",
      "4000 documents loaded\n",
      "5000 documents loaded\n",
      "6000 documents loaded\n",
      "7000 documents loaded\n",
      "8000 documents loaded\n",
      "9000 documents loaded\n",
      "10000 documents loaded\n",
      "11000 documents loaded\n",
      "12000 documents loaded\n",
      "13000 documents loaded\n",
      "14000 documents loaded\n",
      "15000 documents loaded\n",
      "16000 documents loaded\n",
      "17000 documents loaded\n",
      "18000 documents loaded\n",
      "19000 documents loaded\n",
      "20000 documents loaded\n",
      "21000 documents loaded\n",
      "22000 documents loaded\n",
      "23000 documents loaded\n",
      "24000 documents loaded\n",
      "Corpus(documents=25000, tokens=1420894, words=5896238))\n",
      "1000 documents written\n",
      "2000 documents written\n",
      "3000 documents written\n",
      "4000 documents written\n",
      "5000 documents written\n",
      "6000 documents written\n",
      "7000 documents written\n",
      "8000 documents written\n",
      "9000 documents written\n",
      "10000 documents written\n",
      "11000 documents written\n",
      "12000 documents written\n",
      "13000 documents written\n",
      "14000 documents written\n",
      "15000 documents written\n",
      "16000 documents written\n",
      "17000 documents written\n",
      "18000 documents written\n",
      "19000 documents written\n",
      "20000 documents written\n",
      "21000 documents written\n",
      "22000 documents written\n",
      "23000 documents written\n",
      "24000 documents written\n",
      "1000 documents loaded\n",
      "2000 documents loaded\n",
      "3000 documents loaded\n",
      "4000 documents loaded\n",
      "5000 documents loaded\n",
      "6000 documents loaded\n",
      "7000 documents loaded\n",
      "8000 documents loaded\n",
      "9000 documents loaded\n",
      "10000 documents loaded\n",
      "11000 documents loaded\n",
      "12000 documents loaded\n",
      "13000 documents loaded\n",
      "14000 documents loaded\n",
      "15000 documents loaded\n",
      "16000 documents loaded\n",
      "17000 documents loaded\n",
      "18000 documents loaded\n",
      "19000 documents loaded\n",
      "20000 documents loaded\n",
      "21000 documents loaded\n",
      "22000 documents loaded\n",
      "23000 documents loaded\n",
      "24000 documents loaded\n",
      "Corpus(documents=25000, tokens=1384836, words=5775635))\n",
      "1000 documents written\n",
      "2000 documents written\n",
      "3000 documents written\n",
      "4000 documents written\n",
      "5000 documents written\n",
      "6000 documents written\n",
      "7000 documents written\n",
      "8000 documents written\n",
      "9000 documents written\n",
      "10000 documents written\n",
      "11000 documents written\n",
      "12000 documents written\n",
      "13000 documents written\n",
      "14000 documents written\n",
      "15000 documents written\n",
      "16000 documents written\n",
      "17000 documents written\n",
      "18000 documents written\n",
      "19000 documents written\n",
      "20000 documents written\n",
      "21000 documents written\n",
      "22000 documents written\n",
      "23000 documents written\n",
      "24000 documents written\n"
     ]
    }
   ],
   "source": [
    "vocabulary = Encoder.open('movie-review-HW2/aclImdb/imdb.vocab')\n",
    "print(vocabulary)\n",
    "\n",
    "train_corpus = Corpus.open('movie-review-HW2/aclImdb/train/**/*.txt',\n",
    "    ngrams=2,\n",
    "    vocabulary=vocabulary,\n",
    "    verbose=True,\n",
    ")\n",
    "print(train_corpus)\n",
    "train_corpus.write('movie-review-HW2/aclImdb/train-BOW.2ngrams.NB', verbose=True)\n",
    "\n",
    "test_corpus = Corpus.open('movie-review-HW2/aclImdb/test/**/*.txt',\n",
    "    ngrams=2,\n",
    "    vocabulary=vocabulary,\n",
    "    verbose=True\n",
    ")\n",
    "print(test_corpus)\n",
    "test_corpus.write('movie-review-HW2/aclImdb/test-BOW.2ngrams.NB', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus(documents=25000, tokens=1420894, words=5896238))\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vocabulary = Encoder(list(train_corpus.frequencies.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(tokens=1420894, sample=['growing sadly', 'overall horror', '84 and', 'older shows', 'was dirtying', 'old re-runs'])\n"
     ]
    }
   ],
   "source": [
    "print(bigram_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.87928, 'confusion': [[11532, 968], [2050, 10450]]}\n"
     ]
    }
   ],
   "source": [
    "from src.metrics import Metrics\n",
    "\n",
    "predictions = model.predict(test_corpus)\n",
    "score = Metrics.score(test_corpus.labels(), labeler.decode(predictions), labeler)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg', 'pos'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(tokens=2, sample=['neg', 'pos'])\n",
      "Encoder(tokens=89527, sample=['virology', 'hi-jacking', 'holidays', 'sachs', 'loring', 'solarisation'])\n",
      "1000 documents loaded\n",
      "2000 documents loaded\n",
      "3000 documents loaded\n",
      "4000 documents loaded\n",
      "5000 documents loaded\n",
      "6000 documents loaded\n",
      "7000 documents loaded\n",
      "8000 documents loaded\n",
      "9000 documents loaded\n",
      "10000 documents loaded\n",
      "11000 documents loaded\n",
      "12000 documents loaded\n",
      "13000 documents loaded\n",
      "14000 documents loaded\n",
      "15000 documents loaded\n",
      "16000 documents loaded\n",
      "17000 documents loaded\n",
      "18000 documents loaded\n",
      "19000 documents loaded\n",
      "20000 documents loaded\n",
      "21000 documents loaded\n",
      "22000 documents loaded\n",
      "23000 documents loaded\n",
      "24000 documents loaded\n",
      "Corpus(documents=25000, tokens=1420894, words=5896238))\n",
      "1000 documents loaded\n",
      "2000 documents loaded\n",
      "3000 documents loaded\n",
      "4000 documents loaded\n",
      "5000 documents loaded\n",
      "6000 documents loaded\n",
      "7000 documents loaded\n",
      "8000 documents loaded\n",
      "9000 documents loaded\n",
      "10000 documents loaded\n",
      "11000 documents loaded\n",
      "12000 documents loaded\n",
      "13000 documents loaded\n",
      "14000 documents loaded\n",
      "15000 documents loaded\n",
      "16000 documents loaded\n",
      "17000 documents loaded\n",
      "18000 documents loaded\n",
      "19000 documents loaded\n",
      "20000 documents loaded\n",
      "21000 documents loaded\n",
      "22000 documents loaded\n",
      "23000 documents loaded\n",
      "24000 documents loaded\n",
      "Corpus(documents=25000, tokens=1384836, words=5775635))\n"
     ]
    }
   ],
   "source": [
    "labeler = Encoder(['pos', 'neg'])\n",
    "print(labeler)\n",
    "vocabulary = Encoder.open('movie-review-HW2/aclImdb/imdb.vocab')\n",
    "print(vocabulary)\n",
    "\n",
    "train_corpus = Corpus.open('movie-review-HW2/aclImdb/train-BOW.2ngrams.NB', frequencies=True, verbose=True)\n",
    "print(train_corpus)\n",
    "test_corpus = Corpus.open('movie-review-HW2/aclImdb/test-BOW.2ngrams.NB', frequencies=True, verbose=True) \n",
    "print(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 documents fitted\n",
      "2000 documents fitted\n",
      "3000 documents fitted\n",
      "4000 documents fitted\n",
      "5000 documents fitted\n",
      "6000 documents fitted\n",
      "7000 documents fitted\n",
      "8000 documents fitted\n",
      "9000 documents fitted\n",
      "10000 documents fitted\n",
      "11000 documents fitted\n",
      "12000 documents fitted\n",
      "13000 documents fitted\n",
      "14000 documents fitted\n",
      "15000 documents fitted\n",
      "16000 documents fitted\n",
      "17000 documents fitted\n",
      "18000 documents fitted\n",
      "19000 documents fitted\n",
      "20000 documents fitted\n",
      "21000 documents fitted\n",
      "22000 documents fitted\n",
      "23000 documents fitted\n",
      "24000 documents fitted\n",
      "Model(vocabulary=Encoder(tokens=1420894, sample=['growing sadly', 'overall horror', '84 and', 'older shows', 'was dirtying', 'old re-runs']), labeler=Encoder(tokens=2, sample=['neg', 'pos']))\n"
     ]
    }
   ],
   "source": [
    "vocabulary = Encoder(list(train_corpus.frequencies.keys()))\n",
    "model = Model(vocabulary, labeler, log=True)\n",
    "model.fit(train_corpus, train_corpus.labels(), verbose=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdbc323321bfd8046cd6c0131103a4b074176d3dfb408b8d3ec0f56e167f360e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('3.10.0': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
