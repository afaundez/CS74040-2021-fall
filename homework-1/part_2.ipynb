{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afaundez/CS74040-2021-fall/blob/homework-1/homework-1/part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PART II:\n",
        "\n",
        "In this assignment, you will train several language models and will evaluate them on a test corpus. You can discuss in groups, but the homework is to be completed and submitted individually. Two files are provided with this assignment:\n",
        "\n",
        "1. train.txt\n",
        "2. test.txt\n",
        "\n",
        "Each file is a collection of texts, one sentence per line. train.txt contains 10,000 sentences from the NewsCrawl corpus. You will use this corpus to train the language models. The test corpus test.txt is from the same domain and will be used to evaluate the language models that you trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "4IS01d8vtbI0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN Man charged over drugs seizure\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "train must have 10000 sentences, got 100000 instead",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-215-e52d3f43e1a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TRAIN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'train must have 10000 sentences, got {len(train_sentences)} instead'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: train must have 10000 sentences, got 100000 instead"
          ]
        }
      ],
      "source": [
        "train_sentences = open('train.txt').read().strip().split('\\n')\n",
        "print('TRAIN', train_sentences[1])\n",
        "assert len(train_sentences) == 10000, f'train must have 10000 sentences, got {len(train_sentences)} instead'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST The road was pitted with tank treads .\n"
          ]
        }
      ],
      "source": [
        "test_sentences = open('test.txt').read().strip().split('\\n')\n",
        "print('TEST', test_sentences[2])\n",
        "assert len(test_sentences) == 100, f'test must have 100 sentences, got {len(test_sentences)} instead'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 PRE-PROCESSING\n",
        "\n",
        "Prior to training, please complete the following pre-processing steps:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Pad each sentence in the training and test corpora with start and end symbols (you can use \\<s> and \\</s>, respectively)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_sentence(sentence, start_pad = '<s>', stop_pad = '</s>'):\n",
        "    return ' '.join([start_pad] + sentence.split() + [stop_pad])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN <s> Man charged over drugs seizure </s>\n",
            "TEST <s> The road was pitted with tank treads . </s>\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "padded_sentence_pattern = re.compile(\"^<s> .* </s>$\")\n",
        "\n",
        "padded_train_sentences = [ pad_sentence(sentence) for sentence in train ]\n",
        "print('TRAIN', padded_train_sentences[1])\n",
        "assert padded_sentence_pattern.match(padded_train_sentences[1]), f'padded_train_sentences[0] must start with <s> and end with </s>, got {padded_train_sentences[0]} instead'\n",
        "\n",
        "padded_test_sentences = [pad_sentence(sentence) for sentence in test_sentences]\n",
        "print('TEST', padded_test_sentences[2])\n",
        "assert padded_sentence_pattern.match(padded_test_sentences[2]), f'padded_test_sentences[0] must start with <s> and end with </s>, got {padded_test_sentences[0]} instead'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Lowercase all words in the training and test corpora. Note that the data already has been tokenized (i.e. the punctuation has been split off words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN <s> man charged over drugs seizure </s>\n",
            "TEST <s> the road was pitted with tank treads . </s>\n"
          ]
        }
      ],
      "source": [
        "lowercased_padded_train_sentences = [ sentence.lower() for sentence in padded_train_sentences ]\n",
        "print('TRAIN', lowercased_padded_train_sentences[1])\n",
        "assert lowercased_padded_train_sentences[1] == '<s> man charged over drugs seizure </s>', f'lowercased_padded_train_sentences[1] must be \"<s> man charged over drugs seizure </s>\", got {lowercased_padded_train_sentences[1]} instead'\n",
        "\n",
        "lowercased_padded_test_sentences = [ sentence.lower() for sentence in padded_test_sentences ]\n",
        "print('TEST', lowercased_padded_test_sentences[2])\n",
        "assert lowercased_padded_test_sentences[2] == '<s> the road was pitted with tank treads . </s>', f'lowercased_padded_test_sentences[2] must be \"<s> the road was pitted with tank treads . </s>\", got {lowercased_padded_test[1]} instead'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Replace all words occurring in the training data once with the token \\<unk>. Everyword in the test data not seen in training should be treated as \\<unk>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83045\n"
          ]
        }
      ],
      "source": [
        "train_tokens_count_by_token = {}\n",
        "for sentence in lowercased_padded_train_sentences:\n",
        "    for word in sentence.split():\n",
        "        if word not in train_tokens_count_by_token:\n",
        "            train_tokens_count_by_token[word] = 0\n",
        "        train_tokens_count_by_token[word] += 1\n",
        "print(len(train_tokens_count_by_token))\n",
        "assert train_tokens_count_by_token['<s>'] == 100000, f'start token must be 10000, instead got {train_tokens_count_by_token[\"<s>\"]}'\n",
        "\n",
        "once_ocurring_words = set(word for word in train_tokens_count_by_token if train_tokens_count_by_token[word] == 1)\n",
        "\n",
        "assert 'chasnoff' in once_ocurring_words, 'chasnoff must be in once_ocurring_words'\n",
        "\n",
        "def replace_word(sentence, words, token = '<unk>'):\n",
        "    return ' '.join([\n",
        "        token if word in words else word\n",
        "        for word in sentence.split()\n",
        "    ])\n",
        "\n",
        "assert 'chasnoff' in lowercased_padded_train_sentences[2], f'chasnoff must be in {lowercased_padded_train_sentences[2]}'\n",
        "replaced_lowercased_padded_train_sentences = [ replace_word(sentence, once_ocurring_words) for sentence in lowercased_padded_train_sentences ]\n",
        "assert 'chasnoff' not in replaced_lowercased_padded_train_sentences[2], f'chasnoff must not be in {replaced_lowercased_padded_train_sentences[2]}'\n",
        "assert '<unk>' in replaced_lowercased_padded_train_sentences[2], f'<unk> must be in {replaced_lowercased_padded_train_sentences[2]}'\n",
        "\n",
        "replaced_lowercased_padded_train_vocabulary = set(word for sentence in replaced_lowercased_padded_train_sentences for word in sentence.split())\n",
        "assert 'leuthard' not in replaced_lowercased_padded_train_vocabulary, 'leuthard is not in train vocabulary'\n",
        "assert 'septa' not in replaced_lowercased_padded_train_vocabulary, 'septa is not in train vocabulary (replaced with <unk>)'\n",
        "\n",
        "lowercased_padded_test_vocabulary = set(word for sentence in lowercased_padded_test_sentences for word in sentence.split())\n",
        "assert 'leuthard' in lowercased_padded_test_vocabulary, 'leuthard is in test vocabulary'\n",
        "assert 'septa' in lowercased_padded_test_vocabulary, 'septa is in test vocabulary'\n",
        "\n",
        "test_words_not_in_replaced_lowercased_padded_train_vocabulary = lowercased_padded_test_vocabulary.difference(replaced_lowercased_padded_train_vocabulary)\n",
        "assert 'leuthard' in test_words_not_in_replaced_lowercased_padded_train_vocabulary, 'leuthard is in test but not in train vocabulary'\n",
        "assert 'septa' in test_words_not_in_replaced_lowercased_padded_train_vocabulary, 'leuthard is in test but not in train vocabulary'\n",
        "\n",
        "replaced_lowercased_padded_test_sentences = [ replace_word(sentence, test_words_not_in_replaced_lowercased_padded_train_vocabulary) for sentence in lowercased_padded_test_sentences ]\n",
        "assert 'leuthard' not in replaced_lowercased_padded_train_vocabulary, 'leuthard is not in train vocabulary'\n",
        "assert 'septa' not in replaced_lowercased_padded_test_sentences[86], f'septa is not in \"{replaced_lowercased_padded_test_sentences[86]}\" (replaced with <unk>)'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 TRAINING THE MODELS\n",
        "\n",
        "Please use train.txt to train the following language models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 1. A unigram maximum likelihood model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41739 different unigrams\n",
            "2568210 unigrams instances\n",
            "1.000000000000187\n"
          ]
        }
      ],
      "source": [
        "train_unigrams_count_by_unigram = {} # unigram -> count\n",
        "for sentence in replaced_lowercased_padded_train_sentences:\n",
        "    for unigram in sentence.split():\n",
        "        if unigram not in train_unigrams_count_by_unigram:\n",
        "            train_unigrams_count_by_unigram[unigram] = 0\n",
        "        train_unigrams_count_by_unigram[unigram] += 1\n",
        "\n",
        "print(f'{len(train_unigrams_count_by_unigram)} different unigrams')\n",
        "train_unigrams_count = sum(train_unigrams_count_by_unigram.values()) # total number of unigrams\n",
        "print(f'{train_unigrams_count} unigrams instances')\n",
        "\n",
        "# p(token) = count(token) / total_count\n",
        "MLE_probabilities_by_unigram = {\n",
        "    unigram: count / train_unigrams_count\n",
        "    for unigram, count in train_unigrams_count_by_unigram.items()\n",
        "} # unigram -> MLE probability\n",
        "\n",
        "print(sum(MLE_probabilities_by_unigram.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. A bigram maximum likelihood model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import tee\n",
        "\n",
        "def pairwise(iterable):\n",
        "    a, b = tee(iterable)\n",
        "    next(b, None)\n",
        "    return zip(a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "742294 different bigrams\n",
            "2468210 bigrams instances\n"
          ]
        }
      ],
      "source": [
        "train_bigrams_count_by_bigram = {} # bigram -> count\n",
        "for sentence in replaced_lowercased_padded_train_sentences:\n",
        "    for bigram in pairwise(sentence.split()):\n",
        "        if bigram not in train_bigrams_count_by_bigram:\n",
        "            train_bigrams_count_by_bigram[bigram] = 0\n",
        "        train_bigrams_count_by_bigram[bigram] += 1\n",
        "\n",
        "print(f'{len(train_bigrams_count_by_bigram)} different bigrams')\n",
        "train_bigrams_count = sum(train_bigrams_count_by_bigram.values()) # total number of bigrams\n",
        "print(f'{train_bigrams_count} bigrams instances')\n",
        "\n",
        "assert train_bigrams_count_by_bigram[('your', '401')] == 1, f'(\\'your\\', \\'401\\') bigram count must be 1, instead got {train_bigrams_count_by_bigram[(\"your\", \"401\")]}'\n",
        "\n",
        "# p(token | condition) = count(condition, token) / count(condition)\n",
        "MLE_probabilities_by_token_given_condition = {\n",
        "    (token, condition): condition_token_count / train_unigrams_count_by_unigram[condition]\n",
        "    for (condition, token), condition_token_count in train_bigrams_count_by_bigram.items()\n",
        "} # bigram -> MLE joint probability\n",
        "\n",
        "# p(condition, token) = p(condition | token) * p(token)\n",
        "MLE_bigram_joint_probabilities = {\n",
        "    (condition, token): MLE_probabilities_by_token_given_condition[(condition, token)] * MLE_probabilities_by_unigram[token]\n",
        "    for (condition, token) in MLE_probabilities_by_token_given_condition.keys()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "assert sum([ value for (token, _), value in MLE_probabilities_by_token_given_condition.items() if token == '<s>' ]) == 0\n",
        "assert train_unigrams_count_by_unigram['401'] == 15, 'there are 15 instances of token 401 in train'\n",
        "assert MLE_probabilities_by_unigram['401'] == train_unigrams_count_by_unigram['401'] / train_unigrams_count\n",
        "assert set(condition for (condition, token) in train_bigrams_count_by_bigram if token == '401' ) == set(['your', 'corp.', 'their', ',', 'of', 'his', ';', 'a', 'a', 'most', 'in', 'of', 'the', 'his', '<s>'])\n",
        "assert math.isclose(sum([ value * MLE_probabilities_by_unigram[condition] for (token, condition), value in MLE_probabilities_by_token_given_condition.items() if token == '401' ]), MLE_probabilities_by_unigram['401'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. A bigram model with Add-One smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. A bigram model with discounting and Katz backoff. Please use a discount constant of 0.5 (see lecture on smoothing)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPu+5nqDTetxfSaS64LcZBi",
      "include_colab_link": true,
      "name": "part-2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
