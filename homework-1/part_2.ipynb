{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afaundez/CS74040-2021-fall/blob/homework-1/homework-1/part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PART I:\n",
        "\n",
        "(10 points) Do exercise 3.4 from Chapter 3 in the textbook: [https://web.stanford.edu/~jurafsky/slp3/3.pdf](https://web.stanford.edu/~jurafsky/slp3/3.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are given the following corpus, modified from the one in the chapter:\n",
        "\n",
        "    <s> I am Sam </s>\n",
        "    <s> Sam I am </s>\n",
        "    <s> I am Sam </s>\n",
        "    <s> I do not like green eggs and Sam </s>\n",
        "\n",
        "Using a bigram language model with add-one smoothing, what is P(Sam | am)? Include \\<s> and \\</s> in your counts just like any other token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences = ['<s> I am Sam </s>', '<s> Sam I am </s>', '<s> I am Sam </s>', '<s> I do not like green eggs and Sam </s>']\n",
        "sentences = [ sentence.split(' ') for sentence in sentences ]\n",
        "\n",
        "unigrams_count_by_unigram = {}\n",
        "for sentence in sentences:\n",
        "    for word in sentence:\n",
        "        if word not in unigrams_count_by_unigram:\n",
        "            unigrams_count_by_unigram[word] = 0\n",
        "        unigrams_count_by_unigram[word] += 1\n",
        "\n",
        "vocabulary = set(unigrams_count_by_unigram.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import tee\n",
        "\n",
        "def pairwise(iterable):\n",
        "    a, b = tee(iterable)\n",
        "    next(b, None)\n",
        "    return zip(a, b)\n",
        "\n",
        "bigrams_count_by_bigram = {}\n",
        "for sentence in sentences:\n",
        "    for bigram in pairwise(sentence):\n",
        "        if bigram not in bigrams_count_by_bigram:\n",
        "            bigrams_count_by_bigram[bigram] = 0\n",
        "        bigrams_count_by_bigram[bigram] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.21428571428571427"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "MLE_probabilities_by_token_given_condition = {\n",
        "    (token, condition): (condition_token_count + 1) / (unigrams_count_by_unigram[condition] + len(vocabulary))\n",
        "    for (condition, token), condition_token_count in bigrams_count_by_bigram.items()\n",
        "}\n",
        "\n",
        "MLE_probabilities_by_token_given_condition = defaultdict(lambda: (1 / len(vocabulary)), MLE_probabilities_by_token_given_condition)\n",
        "\n",
        "MLE_probabilities_by_token_given_condition[('Sam', 'am')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PART II:\n",
        "\n",
        "In this assignment, you will train several language models and will evaluate them on a test corpus. You can discuss in groups, but the homework is to be completed and submitted individually. Two files are provided with this assignment:\n",
        "\n",
        "1. train.txt\n",
        "2. test.txt\n",
        "\n",
        "Each file is a collection of texts, one sentence per line. train.txt contains 10,000 sentences from the NewsCrawl corpus. You will use this corpus to train the language models. The test corpus test.txt is from the same domain and will be used to evaluate the language models that you trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 PRE-PROCESSING\n",
        "\n",
        "Prior to training, please complete the following pre-processing steps:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Pad each sentence in the training and test corpora with start and end symbols (you can use \\<s> and \\</s>, respectively)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4IS01d8vtbI0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN Man charged over drugs seizure\n"
          ]
        }
      ],
      "source": [
        "train_sentences = open('train.txt').read().strip().split('\\n')\n",
        "print('TRAIN', train_sentences[1])\n",
        "assert len(train_sentences) == 100000, f'train must have 10000 sentences, got {len(train_sentences)} instead'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST The road was pitted with tank treads .\n"
          ]
        }
      ],
      "source": [
        "test_sentences = open('test.txt').read().strip().split('\\n')\n",
        "print('TEST', test_sentences[2])\n",
        "assert len(test_sentences) == 100, f'test must have 100 sentences, got {len(test_sentences)} instead'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_sentence(sentence, start_pad = '<s>', stop_pad = '</s>'):\n",
        "    return ' '.join([start_pad] + sentence.split() + [stop_pad])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN Man charged over drugs seizure -> <s> Man charged over drugs seizure </s>\n",
            "TEST If you have owned the property for more than three years , you can apply for \" taper relief , \" by which you can reduce any taxable gain by 5% for each year of ownership , up to a maximum 40% . -> <s> The road was pitted with tank treads . </s>\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "padded_sentence_pattern = re.compile(\"^<s> .* </s>$\")\n",
        "\n",
        "padded_train_sentences = [ pad_sentence(sentence) for sentence in train_sentences ]\n",
        "print('TRAIN', train_sentences[1], '->', padded_train_sentences[1])\n",
        "assert padded_sentence_pattern.match(padded_train_sentences[1]), f'padded_train_sentences[0] must start with <s> and end with </s>, got {padded_train_sentences[0]} instead'\n",
        "\n",
        "padded_test_sentences = [pad_sentence(sentence) for sentence in test_sentences]\n",
        "print('TEST', test_sentences[1], '->', padded_test_sentences[2])\n",
        "assert padded_sentence_pattern.match(padded_test_sentences[2]), f'padded_test_sentences[0] must start with <s> and end with </s>, got {padded_test_sentences[0]} instead'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Lowercase all words in the training and test corpora. Note that the data already has been tokenized (i.e. the punctuation has been split off words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN <s> Man charged over drugs seizure </s> -> <s> man charged over drugs seizure </s>\n",
            "TEST <s> The road was pitted with tank treads . </s> -> <s> the road was pitted with tank treads . </s>\n"
          ]
        }
      ],
      "source": [
        "lowercased_padded_train_sentences = [ sentence.lower() for sentence in padded_train_sentences ]\n",
        "print('TRAIN', padded_train_sentences[1], '->', lowercased_padded_train_sentences[1])\n",
        "assert lowercased_padded_train_sentences[1] == '<s> man charged over drugs seizure </s>', f'lowercased_padded_train_sentences[1] must be \"<s> man charged over drugs seizure </s>\", got {lowercased_padded_train_sentences[1]} instead'\n",
        "lowercased_padded_train_vocabulary = set(word for sentence in lowercased_padded_train_sentences for word in sentence.split())\n",
        "\n",
        "lowercased_padded_test_sentences = [ sentence.lower() for sentence in padded_test_sentences ]\n",
        "print('TEST', padded_test_sentences[2], '->', lowercased_padded_test_sentences[2])\n",
        "assert lowercased_padded_test_sentences[2] == '<s> the road was pitted with tank treads . </s>', f'lowercased_padded_test_sentences[2] must be \"<s> the road was pitted with tank treads . </s>\", got {lowercased_padded_test[1]} instead'\n",
        "lowercased_padded_test_vocabulary = set(word for sentence in lowercased_padded_test_sentences for word in sentence.split())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Replace all words occurring in the training data once with the token \\<unk>. Everyword in the test data not seen in training should be treated as \\<unk>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83045\n"
          ]
        }
      ],
      "source": [
        "train_tokens_count_by_token = {}\n",
        "for sentence in lowercased_padded_train_sentences:\n",
        "    for word in sentence.split():\n",
        "        if word not in train_tokens_count_by_token:\n",
        "            train_tokens_count_by_token[word] = 0\n",
        "        train_tokens_count_by_token[word] += 1\n",
        "\n",
        "print(len(train_tokens_count_by_token))\n",
        "assert train_tokens_count_by_token['<s>'] == 100000, f'start token must be 10000, instead got {train_tokens_count_by_token[\"<s>\"]}'\n",
        "\n",
        "once_ocurring_words = set(word for word in train_tokens_count_by_token if train_tokens_count_by_token[word] == 1)\n",
        "\n",
        "assert 'chasnoff' in once_ocurring_words, 'chasnoff must be in once_ocurring_words'\n",
        "\n",
        "def replace_word(sentence, words, token = '<unk>'):\n",
        "    return ' '.join([\n",
        "        token if word in words else word\n",
        "        for word in sentence.split()\n",
        "    ])\n",
        "\n",
        "assert 'chasnoff' in lowercased_padded_train_sentences[2], f'chasnoff must be in {lowercased_padded_train_sentences[2]}'\n",
        "replaced_lowercased_padded_train_sentences = [ replace_word(sentence, once_ocurring_words) for sentence in lowercased_padded_train_sentences ]\n",
        "assert 'chasnoff' not in replaced_lowercased_padded_train_sentences[2], f'chasnoff must not be in {replaced_lowercased_padded_train_sentences[2]}'\n",
        "assert '<unk>' in replaced_lowercased_padded_train_sentences[2], f'<unk> must be in {replaced_lowercased_padded_train_sentences[2]}'\n",
        "replaced_lowercased_padded_train_vocabulary = set(word for sentence in replaced_lowercased_padded_train_sentences for word in sentence.split())\n",
        "\n",
        "replaced_lowercased_padded_train_vocabulary = set(word for sentence in replaced_lowercased_padded_train_sentences for word in sentence.split())\n",
        "assert 'leuthard' not in replaced_lowercased_padded_train_vocabulary, 'leuthard is not in train vocabulary'\n",
        "assert 'septa' not in replaced_lowercased_padded_train_vocabulary, 'septa is not in train vocabulary (replaced with <unk>)'\n",
        "replaced_lowercased_padded_train_vocabulary = set(word for sentence in replaced_lowercased_padded_train_sentences for word in sentence.split())\n",
        "\n",
        "lowercased_padded_test_vocabulary = set(word for sentence in lowercased_padded_test_sentences for word in sentence.split())\n",
        "assert 'leuthard' in lowercased_padded_test_vocabulary, 'leuthard is in test vocabulary'\n",
        "assert 'septa' in lowercased_padded_test_vocabulary, 'septa is in test vocabulary'\n",
        "\n",
        "test_words_not_in_replaced_lowercased_padded_train_vocabulary = lowercased_padded_test_vocabulary.difference(replaced_lowercased_padded_train_vocabulary)\n",
        "assert 'leuthard' in test_words_not_in_replaced_lowercased_padded_train_vocabulary, 'leuthard is in test but not in train vocabulary'\n",
        "assert 'septa' in test_words_not_in_replaced_lowercased_padded_train_vocabulary, 'leuthard is in test but not in train vocabulary'\n",
        "\n",
        "replaced_lowercased_padded_test_sentences = [ replace_word(sentence, test_words_not_in_replaced_lowercased_padded_train_vocabulary) for sentence in lowercased_padded_test_sentences ]\n",
        "assert 'leuthard' not in replaced_lowercased_padded_train_vocabulary, 'leuthard is not in train vocabulary'\n",
        "assert 'septa' not in replaced_lowercased_padded_test_sentences[86], f'septa is not in \"{replaced_lowercased_padded_test_sentences[86]}\" (replaced with <unk>)'\n",
        "replaced_lowercased_padded_test_vocabulary = set(word for sentence in replaced_lowercased_padded_test_sentences for word in sentence.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import tee\n",
        "\n",
        "def pairwise(iterable):\n",
        "    a, b = tee(iterable)\n",
        "    next(b, None)\n",
        "    return zip(a, b)\n",
        "\n",
        "def sentenceToWords(sentence, startToken='<s>', stopToken='</s>', unknownToken='<unk>', unknownWords=[]):\n",
        "    words = sentence.strip().lower().split(' ')\n",
        "    words = [\n",
        "        word if word not in unknownWords else unknownToken\n",
        "        for word in words\n",
        "    ]\n",
        "    return [startToken, *words, stopToken]\n",
        "\n",
        "def textToSentences(text, startToken='<s>', stopToken='</s>', unknownToken='<unk>', unknownWords=[]):\n",
        "    raw_sentences = text.strip().split('\\n')\n",
        "    unigrams = {}\n",
        "    bigrams = {}\n",
        "    sentences = []\n",
        "    for sentence in raw_sentences:\n",
        "        token_words = sentenceToWords(sentence, startToken, stopToken, unknownToken, unknownWords)\n",
        "        for (condition, word) in pairwise(token_words):\n",
        "            if condition not in bigrams:\n",
        "                bigrams[condition] = {}\n",
        "            if word not in bigrams[condition]:\n",
        "                bigrams[condition][word] = 0\n",
        "            bigrams[condition][word] += 1\n",
        "        for token_type in token_words:\n",
        "            if token_type not in unigrams:\n",
        "                unigrams[token_type] = 0\n",
        "            unigrams[token_type] += 1\n",
        "        sentences.append(token_words)\n",
        "    return sentences, unigrams, bigrams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100000\n",
            "83045\n",
            "2568210\n",
            "809973\n",
            "2468210\n"
          ]
        }
      ],
      "source": [
        "train_sentences, train_unigrams, train_bigrams = textToSentences(open('train.txt').read())\n",
        "print(len(train_sentences))\n",
        "print(len(train_unigrams))\n",
        "print(sum(train_unigrams.values()))\n",
        "print(sum(len(condition_word.values()) for condition_word in train_bigrams.values()))\n",
        "print(sum(sum(condition_word.values()) for condition_word in train_bigrams.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "1249\n",
            "2869\n",
            "2421\n",
            "2769\n"
          ]
        }
      ],
      "source": [
        "test_sentences, test_unigrams, test_bigrams = textToSentences(open('test.txt').read())\n",
        "print(len(test_sentences))\n",
        "print(len(test_unigrams))\n",
        "print(sum(test_unigrams.values()))\n",
        "print(sum(len(condition_word.values()) for condition_word in test_bigrams.values()))\n",
        "print(sum(sum(condition_word.values()) for condition_word in test_bigrams.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45\n",
            "46\n",
            "1699\n",
            "2044\n"
          ]
        }
      ],
      "source": [
        "test_only_unigrams = { token_type: token_words for token_type, token_words in test_unigrams.items() if token_type not in train_unigrams }\n",
        "print(len(test_only_unigrams.keys()))\n",
        "print(sum(test_only_unigrams.values()))\n",
        "\n",
        "test_only_bigrams = { condition: { word: count for word, count in word_count.items() if word in train_bigrams[condition]} for condition, word_count in test_bigrams.items() if condition in train_bigrams }\n",
        "print(sum(len(token_count.values()) for token_count in test_only_bigrams.values()))\n",
        "print(sum(sum(token_count.values()) for token_count in test_only_bigrams.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41307\n",
            "41307\n",
            "100000\n",
            "41739\n",
            "2568210\n",
            "742294\n",
            "2468210\n"
          ]
        }
      ],
      "source": [
        "train_once_unigrams = { word: count for word, count in train_unigrams.items() if count == 1 }\n",
        "print(len(train_once_unigrams.keys()))\n",
        "print(sum(train_once_unigrams.values()))\n",
        "\n",
        "train_sentences_with_replacing, train_unigrams_with_replacing, train_bigrams_with_replacing = textToSentences(open('train.txt').read(), unknownWords=train_once_unigrams)\n",
        "print(len(train_sentences_with_replacing))\n",
        "print(len(train_unigrams_with_replacing))\n",
        "print(sum(train_unigrams_with_replacing.values()))\n",
        "print(sum(len(condition_word.values()) for condition_word in train_bigrams_with_replacing.values()))\n",
        "print(sum(sum(condition_word.values()) for condition_word in train_bigrams_with_replacing.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45\n",
            "46\n",
            "100\n",
            "1175\n",
            "2869\n",
            "2365\n",
            "2769\n"
          ]
        }
      ],
      "source": [
        "test_only_unigrams = { word: count for word, count in test_unigrams.items() if word not in train_unigrams }\n",
        "print(len(test_only_unigrams.keys()))\n",
        "print(sum(test_only_unigrams.values()))\n",
        "\n",
        "test_sentences_with_replacing, test_unigrams_with_replacing, test_bigrams_with_replacing = textToSentences(open('test.txt').read(), unknownWords=(set(test_only_unigrams.keys()).union(set(train_once_unigrams.keys()))))\n",
        "print(len(test_sentences_with_replacing))\n",
        "print(len(test_unigrams_with_replacing))\n",
        "print(sum(test_unigrams_with_replacing.values()))\n",
        "print(sum(len(condition_word.values()) for condition_word in test_bigrams_with_replacing.values()))\n",
        "print(sum(sum(condition_word.values()) for condition_word in test_bigrams_with_replacing.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 TRAINING THE MODELS\n",
        "\n",
        "Please use train.txt to train the following language models:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. A unigram maximum likelihood model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(<s>) = count(<s>) / count() = 100000 / 2568210 = 0.03893762581720342\n",
            "P(man) = count(man) / count() = 942 / 2568210 = 0.00036679243519805626\n",
            "P(charged) = count(charged) / count() = 245 / 2568210 = 9.539718325214838e-05\n",
            "P(over) = count(over) / count() = 2876 / 2568210 = 0.0011198461185027704\n",
            "P(drugs) = count(drugs) / count() = 275 / 2568210 = 0.00010707847099730941\n",
            "P(</s>) = count(</s>) / count() = 100000 / 2568210 = 0.03893762581720342\n",
            "P(man charged over drugs) =  P(<s>) * P(man) * P(charged) * P(over) * P(drugs) * P(</s>)  =  0.03893762581720342 * 0.00036679243519805626 * 9.539718325214838e-05 * 0.0011198461185027704 * 0.00010707847099730941 * 0.03893762581720342  =  6.361438993281194e-18 \n",
            "\n",
            "P(<s>) = count(<s>) / count() = 100000 / 2568210 = 0.03893762581720342\n",
            "P(man) = count(man) / count() = 942 / 2568210 = 0.00036679243519805626\n",
            "P(charged) = count(charged) / count() = 245 / 2568210 = 9.539718325214838e-05\n",
            "P(over) = count(over) / count() = 2876 / 2568210 = 0.0011198461185027704\n",
            "P(drugs) = count(drugs) / count() = 275 / 2568210 = 0.00010707847099730941\n",
            "P(lalalllala) = count(lalalllala) / count() = 0 / 2568210 = 0\n",
            "P(</s>) = count(</s>) / count() = 100000 / 2568210 = 0.03893762581720342\n",
            "P(man charged over drugs lalalllala) =  P(<s>) * P(man) * P(charged) * P(over) * P(drugs) * P(lalalllala) * P(</s>)  =  0.03893762581720342 * 0.00036679243519805626 * 9.539718325214838e-05 * 0.0011198461185027704 * 0.00010707847099730941 * 0 * 0.03893762581720342  =  0.0 \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "class UnigramModel:\n",
        "    def __init__(self, unigrams):\n",
        "        self.unigrams = unigrams\n",
        "    \n",
        "    def unigramMLE(self, unigram, log=False):\n",
        "        denominator = sum(self.unigrams.values())\n",
        "        if unigram in self.unigrams:\n",
        "            numerator = self.unigrams[unigram]\n",
        "            result = numerator / denominator\n",
        "        else:\n",
        "            numerator = 0\n",
        "            result = 0\n",
        "        equation = f'count({unigram}) / count() = {numerator} / {denominator}'\n",
        "        return result, equation\n",
        "\n",
        "    def sentenceMLE(self, sentence, log=False):\n",
        "        words = sentenceToWords(sentence)\n",
        "        equation = {}\n",
        "        for unigram in words:\n",
        "            probability, probability_equation = self.unigramMLE(unigram, log)\n",
        "            key = f'P({unigram})'\n",
        "            equation[key] = probability\n",
        "            print(f'{key} = {probability_equation} = {probability}')\n",
        "\n",
        "        result = math.prod(equation.values())\n",
        "        print(f'P({sentence}) = ', ' * '.join(equation.keys()), ' = ', ' * '.join([ str(v) for v in equation.values()]), ' = ', result, '\\n')\n",
        "        return result\n",
        "\n",
        "unigramModel = UnigramModel(train_unigrams_with_replacing)\n",
        "unigramModel.sentenceMLE('man charged over drugs')\n",
        "unigramModel.sentenceMLE('man charged over drugs lalalllala')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. A bigram maximum likelihood model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import math\n",
        "\n",
        "# assert sum([ count_by_tokens['<s>'] for count_by_tokens in MLE_probabilities_by_token_given_condition.values() if '<s>' in count_by_tokens ]) == 0\n",
        "# assert train_unigrams_count_by_unigram['401'] == 15, 'there are 15 instances of token 401 in train'\n",
        "# assert MLE_probabilities_by_unigram['401'] == train_unigrams_count_by_unigram['401'] / train_unigrams_count\n",
        "# assert set(condition for condition, count_by_tokens in train_bigrams_count_by_bigram.items() if '401' in count_by_tokens) == set(['your', 'corp.', 'their', ',', 'of', 'his', ';', 'a', 'a', 'most', 'in', 'of', 'the', 'his', '<s>'])\n",
        "# assert math.isclose(sum([ count_by_tokens['401'] * MLE_probabilities_by_unigram[condition] for condition, count_by_tokens in MLE_probabilities_by_token_given_condition.items() if '401' in count_by_tokens ]), MLE_probabilities_by_unigram['401'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(man | <s>) = 0.00075\n",
            "P(charged | man) = 0.0074309978768577496\n",
            "P(over | charged) = 0.02857142857142857\n",
            "P(drugs | over) = 0.0006954102920723226\n",
            "P(seizure | drugs) = 0.0036363636363636364\n",
            "P(</s> | seizure) = 0.07692307692307693\n",
            "P(Man charged over drugs seizure) =  P(man | <s>) * P(charged | man) * P(over | charged) * P(drugs | over) * P(seizure | drugs) * P(</s> | seizure)  =  0.00075 * 0.0074309978768577496 * 0.02857142857142857 * 0.0006954102920723226 * 0.0036363636363636364 * 0.07692307692307693  =  3.097457984376298e-14\n",
            "\n",
            "P(man | <s>) = 0.00075\n",
            "P(charged | man) = 0.0074309978768577496\n",
            "P(over | charged) = 0.02857142857142857\n",
            "P(drugs | over) = 0.0006954102920723226\n",
            "P(seizure | drugs) = 0.0036363636363636364\n",
            "P(lalalala | seizure) = 0\n",
            "P(</s> | lalalala) = 0\n",
            "P(Man charged over drugs seizure lalalala) =  P(man | <s>) * P(charged | man) * P(over | charged) * P(drugs | over) * P(seizure | drugs) * P(lalalala | seizure) * P(</s> | lalalala)  =  0.00075 * 0.0074309978768577496 * 0.02857142857142857 * 0.0006954102920723226 * 0.0036363636363636364 * 0 * 0  =  0.0\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "class BigramModel:\n",
        "    def __init__(self, unigrams, bigrams):\n",
        "        self.unigrams = unigrams\n",
        "        self.bigrams = bigrams\n",
        "    \n",
        "    def bigramMLE(self, word, condition, log=False):\n",
        "        if condition in self.bigrams and word in self.bigrams[condition]:\n",
        "            numerator = self.bigrams[condition][word]\n",
        "            denominator = self.unigrams[condition]\n",
        "            result = numerator / denominator\n",
        "        else:\n",
        "            numerator = 0\n",
        "            denominator = 0\n",
        "            result = 0\n",
        "        equation = f'count({condition}, {word}) / count({condition}) = {numerator} / {denominator}'\n",
        "        return result, equation\n",
        "\n",
        "    def sentenceMLE(self, sentence, log=False):\n",
        "        words = sentenceToWords(sentence)\n",
        "        bigrams = list(pairwise(words))\n",
        "\n",
        "        equation = {}\n",
        "        for condition, word in bigrams:\n",
        "            probability, probability_equation = self.bigramMLE(word, condition)\n",
        "            key = f'P({word} | {condition})'\n",
        "            equation[key] = probability\n",
        "            print(f'{key} = {probability}')\n",
        "\n",
        "        result = math.prod(equation.values())\n",
        "        print(f'P({sentence}) = ', ' * '.join(equation.keys()), ' = ', ' * '.join([ str(v) for v in equation.values()]), ' = ', result)\n",
        "        print()\n",
        "        return result\n",
        "\n",
        "bigramModel = BigramModel(train_unigrams_with_replacing, train_bigrams_with_replacing)\n",
        "bigramModel.sentenceMLE('Man charged over drugs seizure')\n",
        "bigramModel.sentenceMLE('Man charged over drugs seizure lalalala')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. A bigram model with Add-One smoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(man | <s>) = (count(<s>, man) + 1) / (count(<s>) + |V|) = (75 + 1) / (942 + 41738) = 0.001780693533270853\n",
            "P(charged | man) = (count(man, charged) + 1) / (count(man) + |V|) = (7 + 1) / (245 + 41738) = 0.00019055331920062884\n",
            "P(over | charged) = (count(charged, over) + 1) / (count(charged) + |V|) = (7 + 1) / (2876 + 41738) = 0.00017931590980409736\n",
            "P(drugs | over) = (count(over, drugs) + 1) / (count(over) + |V|) = (2 + 1) / (275 + 41738) = 7.140646942613001e-05\n",
            "P(seizure | drugs) = (count(drugs, seizure) + 1) / (count(drugs) + |V|) = (1 + 1) / (13 + 41738) = 4.7903044238461356e-05\n",
            "P(</s> | seizure) = (count(seizure, </s>) + 1) / (count(seizure) + |V|) = (1 + 1) / (100000 + 41738) = 1.4110541985917679e-05\n",
            "P(Man charged over drugs seizure) =  P(man | <s>) * P(charged | man) * P(over | charged) * P(drugs | over) * P(seizure | drugs) * P(</s> | seizure)  =  0.001780693533270853 * 0.00019055331920062884 * 0.00017931590980409736 * 7.140646942613001e-05 * 4.7903044238461356e-05 * 1.4110541985917679e-05  =  2.936762955331157e-24\n",
            "\n",
            "P(man | <s>) = (count(<s>, man) + 1) / (count(<s>) + |V|) = (75 + 1) / (942 + 41738) = 0.001780693533270853\n",
            "P(charged | man) = (count(man, charged) + 1) / (count(man) + |V|) = (7 + 1) / (245 + 41738) = 0.00019055331920062884\n",
            "P(over | charged) = (count(charged, over) + 1) / (count(charged) + |V|) = (7 + 1) / (2876 + 41738) = 0.00017931590980409736\n",
            "P(drugs | over) = (count(over, drugs) + 1) / (count(over) + |V|) = (2 + 1) / (275 + 41738) = 7.140646942613001e-05\n",
            "P(seizure | drugs) = (count(drugs, seizure) + 1) / (count(drugs) + |V|) = (1 + 1) / (13 + 41738) = 4.7903044238461356e-05\n",
            "P(lalalala | seizure) = (count(seizure, lalalala) + 1) / (count(seizure) + |V|) = (0 + 1) / (0 + 41738) = 2.3958982222435192e-05\n",
            "P(</s> | lalalala) = (count(lalalala, </s>) + 1) / (count(lalalala) + |V|) = (0 + 1) / (100000 + 41738) = 7.0552709929588394e-06\n",
            "P(Man charged over drugs seizure lalalala) =  P(man | <s>) * P(charged | man) * P(over | charged) * P(drugs | over) * P(seizure | drugs) * P(lalalala | seizure) * P(</s> | lalalala)  =  0.001780693533270853 * 0.00019055331920062884 * 0.00017931590980409736 * 7.140646942613001e-05 * 4.7903044238461356e-05 * 2.3958982222435192e-05 * 7.0552709929588394e-06  =  3.5180925719142713e-29\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3.5180925719142713e-29"
            ]
          },
          "execution_count": 204,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "class SmoothBigramModel:\n",
        "    def __init__(self, unigrams, bigrams):\n",
        "        self.unigrams = unigrams\n",
        "        self.bigrams = bigrams\n",
        "    \n",
        "    def bigramMLE(self, word, condition, log=False):\n",
        "        if condition in self.bigrams and word in self.bigrams[condition]:\n",
        "            numerator = self.bigrams[condition][word]\n",
        "        else:\n",
        "            numerator = 0\n",
        "        if word in self.unigrams:\n",
        "            denominator = self.unigrams[word]\n",
        "        else:\n",
        "            denominator = 0\n",
        "        vocab_size = len(self.bigrams.keys())\n",
        "        equation = f'(count({condition}, {word}) + 1) / (count({condition}) + |V|) = ({numerator} + 1) / ({denominator} + {vocab_size})'\n",
        "\n",
        "        result = (numerator + 1) / (denominator + vocab_size)\n",
        "        return result, equation\n",
        "\n",
        "    def sentenceMLE(self, sentence, log=False):\n",
        "        words = sentenceToWords(sentence)\n",
        "        bigrams = list(pairwise(words))\n",
        "\n",
        "        equation = {}\n",
        "        for condition, word in bigrams:\n",
        "            probability, probability_equation = self.bigramMLE(word, condition)\n",
        "            key = f'P({word} | {condition})'\n",
        "            equation[key] = probability\n",
        "            print(f'{key} = {probability_equation} = {probability}')\n",
        "\n",
        "        result = math.prod(equation.values())\n",
        "        print(f'P({sentence}) = ', ' * '.join(equation.keys()), ' = ', ' * '.join([ str(v) for v in equation.values()]), ' = ', result)\n",
        "        print()\n",
        "        return result\n",
        "\n",
        "smoothBigramModel = SmoothBigramModel(train_unigrams_with_replacing, train_bigrams_with_replacing)\n",
        "smoothBigramModel.sentenceMLE('Man charged over drugs seizure')\n",
        "smoothBigramModel.sentenceMLE('Man charged over drugs seizure lalalala')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. A bigram model with discounting and Katz backoff. Please use a discount constant of 0.5 (see lecture on smoothing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(man | <s>) = count*(<s>, man) / count(<s>) = 74.5 / 100000 = 0.000745\n",
            "P(charged | man) = count*(man, charged) / count(man) = 6.5 / 942 = 0.006900212314225053\n",
            "P(over | charged) = count*(charged, over) / count(charged) = 6.5 / 245 = 0.026530612244897958\n",
            "P(drugs | over) = count*(over, drugs) / count(over) = 1.5 / 2876 = 0.000521557719054242\n",
            "P(seizure | drugs) = count*(drugs, seizure) / count(drugs) = 0.5 / 275 = 0.0018181818181818182\n",
            "P(</s> | seizure) = count*(seizure, </s>) / count(seizure) = 0.5 / 13 = 0.038461538461538464\n",
            "P(Man charged over drugs seizure) =  P(man | <s>) * P(charged | man) * P(over | charged) * P(drugs | over) * P(seizure | drugs) * P(</s> | seizure)  =  0.000745 * 0.006900212314225053 * 0.026530612244897958 * 0.000521557719054242 * 0.0018181818181818182 * 0.038461538461538464  =  4.974304177587982e-15\n",
            "\n",
            "P(man | <s>) = count*(<s>, man) / count(<s>) = 74.5 / 100000 = 0.000745\n",
            "P(charged | man) = count*(man, charged) / count(man) = 6.5 / 942 = 0.006900212314225053\n",
            "P(over | charged) = count*(charged, over) / count(charged) = 6.5 / 245 = 0.026530612244897958\n",
            "P(drugs | over) = count*(over, drugs) / count(over) = 1.5 / 2876 = 0.000521557719054242\n",
            "P(seizure | drugs) = count*(drugs, seizure) / count(drugs) = 0.5 / 275 = 0.0018181818181818182\n",
            "alpha(seizure) = 1 - Sigma(count*(seizure, word) / count(seizure)) = 1 - 8.0 / 13 = 0.3846153846153846\n",
            "P(man | seizure) = alpha(seizure) * P(man) / Sigma_w_in_B(P(w)) = 0.3846153846153846 * 0.00036679243519805626 / 0.8823986356257792 = 0.0001598756025247785\n",
            "P(</s> | man) = count*(man, </s>) / count(man) = 5.5 / 942 = 0.00583864118895966\n",
            "P(Man charged over drugs seizure man) =  P(man | <s>) * P(charged | man) * P(over | charged) * P(drugs | over) * P(seizure | drugs) * P(man | seizure) * P(</s> | man)  =  0.000745 * 0.006900212314225053 * 0.026530612244897958 * 0.000521557719054242 * 0.0018181818181818182 * 0.0001598756025247785 * 0.00583864118895966  =  1.2072568204594095e-19\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1.2072568204594095e-19"
            ]
          },
          "execution_count": 231,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class KatzBigramModel:\n",
        "    def __init__(self, unigrams, bigrams):\n",
        "        self.unigrams = unigrams\n",
        "        self.bigrams = bigrams\n",
        "\n",
        "        self.bigrams_star = {}\n",
        "        self.leftovers = {}\n",
        "        for condition, word_count in self.bigrams.items():\n",
        "            self.leftovers[condition] = 1\n",
        "            if condition not in self.bigrams_star:\n",
        "                self.bigrams_star[condition] = {}\n",
        "            for word, count in word_count.items():\n",
        "                if word not in self.bigrams_star[condition]:\n",
        "                    self.bigrams_star[condition][word] = 0\n",
        "                self.bigrams_star[condition][word] = count - 0.5\n",
        "                self.leftovers[condition] += 0.5\n",
        "    \n",
        "    def unigramMLE(self, unigram, log=False):\n",
        "        denominator = sum(self.unigrams.values())\n",
        "        if unigram in self.unigrams:\n",
        "            numerator = self.unigrams[unigram]\n",
        "            result = numerator / denominator\n",
        "        else:\n",
        "            numerator = 0\n",
        "            result = 0\n",
        "        equation = f'count({unigram}) / count() = {numerator} / {denominator}'\n",
        "        return result, equation\n",
        "\n",
        "    def bigramMLE(self, word, condition):\n",
        "        if condition in self.bigrams_star:\n",
        "            A_condition = set(self.bigrams_star[condition].keys())\n",
        "        else:\n",
        "            A_condition = set()\n",
        "\n",
        "        B_condition = set(self.unigrams.keys()) - A_condition\n",
        "        if word in A_condition:\n",
        "            numerator = self.bigrams_star[condition][word]\n",
        "            denominator = self.unigrams[condition]\n",
        "            result = numerator / denominator\n",
        "            return result, f'count*({condition}, {word}) / count({condition}) = {numerator} / {denominator}'\n",
        "        else:\n",
        "            alpha_numerator = sum(self.bigrams_star[condition].values())\n",
        "            alpha_denominator = self.unigrams[condition]\n",
        "            alpha_condition = 1 - alpha_numerator / alpha_denominator\n",
        "            print(f'alpha({condition}) = 1 - Sigma(count*({condition}, word) / count({condition})) = 1 - {alpha_numerator} / {alpha_denominator} = {alpha_condition}')\n",
        "            numerator, _ = self.unigramMLE(word)\n",
        "            denominator = sum(self.unigramMLE(unigram)[0] for unigram in self.unigrams if unigram in B_condition)\n",
        "            result = alpha_condition * numerator / denominator\n",
        "            return result, f'alpha({condition}) * P({word}) / Sigma_w_in_B(P(w)) = {alpha_condition} * {numerator} / {denominator}'\n",
        "\n",
        "\n",
        "    def sentenceMLE(self, sentence, log=False):\n",
        "        words = sentenceToWords(sentence)\n",
        "        bigrams = list(pairwise(words))\n",
        "\n",
        "        equation = {}\n",
        "        for condition, word in bigrams:\n",
        "            probability, probability_equation = self.bigramMLE(word, condition)\n",
        "            key = f'P({word} | {condition})'\n",
        "            equation[key] = probability\n",
        "            print(f'{key} = {probability_equation} = {probability}')\n",
        "\n",
        "        result = math.prod(equation.values())\n",
        "        print(f'P({sentence}) = ', ' * '.join(equation.keys()), ' = ', ' * '.join([ str(v) for v in equation.values()]), ' = ', result)\n",
        "        print()\n",
        "        return result\n",
        "\n",
        "katzBigramModel = KatzBigramModel(train_unigrams_with_replacing, train_bigrams_with_replacing)\n",
        "katzBigramModel.sentenceMLE('Man charged over drugs seizure')\n",
        "katzBigramModel.sentenceMLE('Man charged over drugs seizure man')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 QUESTIONS\n",
        "\n",
        "Please answer the questions below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. (5 points) How many word types (unique words) are there in the training corpus? Please include the end-of-sentence padding symbol \\</s> and the unknown token \\<unk>. Do not include the start of sentence padding symbol \\<s>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train unique words count 41738\n",
            "test unique words count 1174\n"
          ]
        }
      ],
      "source": [
        "train_unigrams = set(train_unigrams_count_by_unigram.keys()) - {'<s>'}\n",
        "print('train unique words count', len(train_unigrams))\n",
        "\n",
        "test_unigrams = set(test_unigrams_count_by_unigram.keys()) - {'<s>'}\n",
        "print('test unique words count', len(test_unigrams))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. (5 points) How many word tokens are there in the training corpus? Do not include the start of sentence padding symbol \\<s>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train unigrams words count 2468210\n",
            "test unigrams words count 2769\n"
          ]
        }
      ],
      "source": [
        "train_unigrams_count = sum([ value for word, value in train_unigrams_count_by_unigram.items() if word not in {'<s>'} ])\n",
        "print('train unigrams words count', train_unigrams_count)\n",
        "\n",
        "test_unigrams_count = sum([ value for word, value in test_unigrams_count_by_unigram.items() if word not in {'<s>'} ])\n",
        "print('test unigrams words count', test_unigrams_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. (10 points) What percentage of word tokens and word types in the test corpus did not occur in training (before you mapped the unknown words to \\<unk> in training and test data)? Please include the padding symbol \\</s> in your calculations. Do not include the start of sentence padding symbol \\<s>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1248 1174\n"
          ]
        }
      ],
      "source": [
        "print(len(lowercased_padded_test_vocabulary - {'<s>'}), len(replaced_lowercased_padded_test_vocabulary - {'<s>'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. (15 points) Now replace singletons in the training data with \\<unk> symbol and map words (in the test corpus) not observed in training to \\<unk>. What percentage of bigrams (bigram types and bigram tokens) in the test corpus did not occur in training (treat \\<unk> as a regular token that has been observed). Please include the padding symbol \\</s> in your calculations. Do not include the start of sentence padding symbol \\<s>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bigrams types in test 2300\n",
            "bigrams types in test but not in train 595 (25.869565217391305%)\n",
            "bigrams words in test 2669\n",
            "bigrams words in test but not in train 597 (22.367928062944923%)\n"
          ]
        }
      ],
      "source": [
        "excluded_unigrams =  {'<s>'}\n",
        "\n",
        "train_bigram_type = {}\n",
        "for sentence in replaced_lowercased_padded_train_sentences:\n",
        "    for bigram in pairwise(sentence.split()):\n",
        "        if bigram[0] in excluded_unigrams:\n",
        "            continue\n",
        "        if bigram not in train_bigram_type:\n",
        "            train_bigram_type[bigram] = 0\n",
        "        train_bigram_type[bigram] += 1\n",
        "\n",
        "test_bigram_types = {}\n",
        "for sentence in replaced_lowercased_padded_test_sentences:\n",
        "    for bigram in pairwise(sentence.split()):\n",
        "        if bigram[0] in excluded_unigrams:\n",
        "            continue\n",
        "        if bigram not in test_bigram_types:\n",
        "            test_bigram_types[bigram] = 0\n",
        "        test_bigram_types[bigram] += 1\n",
        "\n",
        "test_bigram_types_count = len(test_bigram_types)\n",
        "print('bigrams types in test', test_bigram_types_count)\n",
        "\n",
        "only_test_bigram_types = test_bigram_types.keys() - train_bigram_type.keys()\n",
        "only_test_bigram_types_count = len(only_test_bigram_types)\n",
        "print('bigrams types in test but not in train', only_test_bigram_types_count, f'({only_test_bigram_types_count / test_bigram_types_count * 100}%)')\n",
        "\n",
        "\n",
        "test_bigram_words_count = sum(test_bigram_types.values())\n",
        "print('bigrams words in test', sum(test_bigram_types.values()))\n",
        "\n",
        "only_test_bigram_types_words_count = sum([ count for bigram, count in test_bigram_types.items() if bigram in only_test_bigram_types ])\n",
        "print('bigrams words in test but not in train', only_test_bigram_types_words_count, f'({only_test_bigram_types_words_count / test_bigram_words_count*100}%)')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. (15 points) Compute the log probability of the following sentence under the three models (ignore capitalization and pad each sentence as described above). Please list all of the parameters required to compute the probabilities and show the complete calculation. Which of the parameters have zero values under each model? Use log base 2 in your calculations. Map words not observed in the training corpus to the \\<unk> token.\n",
        "\n",
        "- I look forward to hearing your reply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(<s>) = 0.03893762581720342\n",
            "P(i) = 0.0028576323587245593\n",
            "P(look) = 0.000238687646259457\n",
            "P(forward) = 0.00018456434637354423\n",
            "P(to) = 0.02065563174351007\n",
            "P(hearing) = 8.137963795795515e-05\n",
            "P(your) = 0.00047387090619536564\n",
            "P(reply) = 5.061891356236445e-06\n",
            "P(.) = 0.03422383683577278\n",
            "P(</s>) = 0.03893762581720342\n",
            "P(I look forward to hearing your reply .) =  P(<s>) * P(i) * P(look) * P(forward) * P(to) * P(hearing) * P(your) * P(reply) * P(.) * P(</s>)  =  0.03893762581720342 * 0.0028576323587245593 * 0.000238687646259457 * 0.00018456434637354423 * 0.02065563174351007 * 8.137963795795515e-05 * 0.00047387090619536564 * 5.061891356236445e-06 * 0.03422383683577278 * 0.03893762581720342  =  2.633776012975432e-29\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2.633776012975432e-29"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unigram_model('I look forward to hearing your reply .', MLE_probabilities_by_unigram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(i | <s>) = 0.02006\n",
            "P(look | i) = 0.0020438751873552256\n",
            "P(forward | look) = 0.05546492659053834\n",
            "P(to | forward) = 0.2109704641350211\n",
            "P(hearing | to) = 0.00011310511235107827\n",
            "P(your | hearing) = 0\n",
            "P(reply | your) = 0\n",
            "P(. | reply) = 0\n",
            "P(</s> | .) = 0.9430450315152342\n",
            "P(I look forward to hearing your reply .) =  P(i | <s>) * P(look | i) * P(forward | look) * P(to | forward) * P(hearing | to) * P(your | hearing) * P(reply | your) * P(. | reply) * P(</s> | .)  =  0.02006 * 0.0020438751873552256 * 0.05546492659053834 * 0.2109704641350211 * 0.00011310511235107827 * 0 * 0 * 0 * 0.9430450315152342  =  0.0\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_model('I look forward to hearing your reply .', MLE_probabilities_by_token_given_condition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. (20 points) Compute the perplexity of the sentence above under each of the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(i | <s>) = 0.014159828981437713\n",
            "P(look | i) = 0.0003260116549166633\n",
            "P(forward | look) = 0.0008264072534945221\n",
            "P(to | forward) = 0.002392627863454386\n",
            "P(hearing | to) = 7.384978952809984e-05\n",
            "P(your | hearing) = 2.395840820335897e-05\n",
            "P(reply | your) = 2.395840820335897e-05\n",
            "P(. | reply) = 2.395840820335897e-05\n",
            "P(</s> | .) = 0.6394128038385287\n",
            "P(I look forward to hearing your reply .) =  P(i | <s>) * P(look | i) * P(forward | look) * P(to | forward) * P(hearing | to) * P(your | hearing) * P(reply | your) * P(. | reply) * P(</s> | .)  =  0.014159828981437713 * 0.0003260116549166633 * 0.0008264072534945221 * 0.002392627863454386 * 7.384978952809984e-05 * 2.395840820335897e-05 * 2.395840820335897e-05 * 2.395840820335897e-05 * 0.6394128038385287  =  5.9274088158499185e-30\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "5.9274088158499185e-30"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_model('I look forward to hearing your reply .', MLE_laplace_probabilities_by_token_given_condition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(i | <s>) = 0.002857618070562231\n",
            "P(look | i) = 0.00023867138466748843\n",
            "P(forward | look) = 0.00018441380449229155\n",
            "P(to | forward) = 0.02063384310242656\n",
            "P(hearing | to) = 8.137887092018207e-05\n",
            "P(your | hearing) = 0.00047273724374026445\n",
            "P(reply | your) = 5.059811696680759e-06\n",
            "P(. | reply) = 0.03290753541900613\n",
            "P(</s> | .) = 0.038937404313875\n",
            "P(I look forward to hearing your reply .) =  P(i | <s>) * P(look | i) * P(forward | look) * P(to | forward) * P(hearing | to) * P(your | hearing) * P(reply | your) * P(. | reply) * P(</s> | .)  =  0.002857618070562231 * 0.00023867138466748843 * 0.00018441380449229155 * 0.02063384310242656 * 8.137887092018207e-05 * 0.00047273724374026445 * 5.059811696680759e-06 * 0.03290753541900613 * 0.038937404313875  =  6.473009959346732e-28\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6.473009959346732e-28"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_katz_model('I look forward to hearing your reply .', backoff_probabilities, train_discounted_counts_by_bigram, MLE_probabilities_by_unigram, train_unigrams_count_by_unigram)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. (20 points) Compute the perplexity of the entire test corpus under each of the models.Discuss the differences in the results you obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPu+5nqDTetxfSaS64LcZBi",
      "include_colab_link": true,
      "name": "part-2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
