{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PART I:\n",
        "\n",
        "(10 points) Do exercise 3.4 from Chapter 3 in the textbook: [https://web.stanford.edu/~jurafsky/slp3/3.pdf](https://web.stanford.edu/~jurafsky/slp3/3.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are given the following corpus, modified from the one in the chapter:\n",
        "\n",
        "    <s> I am Sam </s>\n",
        "    <s> Sam I am </s>\n",
        "    <s> I am Sam </s>\n",
        "    <s> I do not like green eggs and Sam </s>\n",
        "\n",
        "Using a bigram language model with add-one smoothing, what is P(Sam | am)? Include \\<s> and \\</s> in your counts just like any other token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 741,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(\\texttt{am} \\mid \\texttt{sam}) &=  \\frac{count^{*}(\\texttt{am} , \\texttt{sam}) + 1}{count(am) + |V|} = \\frac{2 + 1}{3 + 10} = 0.23076923076923078  \\\\\n"
          ]
        }
      ],
      "source": [
        "p1_corpora = filenameToCorpora('p1.txt', startToken='<s>', stopToken='</s>')\n",
        "p1_unigrams, p1_bigrams = processGrams(p1_corpora)\n",
        "p1Model = SmoothBigramModel(p1_unigrams, p1_bigrams)\n",
        "output = p1Model.bigramMLE('sam', 'am', log=False, verbose=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PART II:\n",
        "\n",
        "In this assignment, you will train several language models and will evaluate them on a test corpus. You can discuss in groups, but the homework is to be completed and submitted individually. Two files are provided with this assignment:\n",
        "\n",
        "1. train.txt\n",
        "2. test.txt\n",
        "\n",
        "Each file is a collection of texts, one sentence per line. train.txt contains 10,000 sentences from the NewsCrawl corpus. You will use this corpus to train the language models. The test corpus test.txt is from the same domain and will be used to evaluate the language models that you trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 PRE-PROCESSING\n",
        "\n",
        "Prior to training, please complete the following pre-processing steps:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Pad each sentence in the training and test corpora with start and end symbols (you can use \\<s> and \\</s>, respectively)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadSentences(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        corpora = f.read().strip()\n",
        "        return [ sentence.strip().split(' ') for sentence in corpora.split('\\n') ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4IS01d8vtbI0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN ['Man', 'charged', 'over', 'drugs', 'seizure']\n",
            "TEST ['The', 'road', 'was', 'pitted', 'with', 'tank', 'treads', '.']\n"
          ]
        }
      ],
      "source": [
        "train_corpora = loadSentences('data/train.txt')\n",
        "print('TRAIN', train_corpora[1])\n",
        "assert len(train_corpora) == 100000, f'train must have 10000 sentences, got {len(train_corpora)} instead'\n",
        "\n",
        "test_corpora = loadSentences('data/test.txt')\n",
        "print('TEST', test_corpora[2])\n",
        "assert len(test_corpora) == 100, f'test must have 100 sentences, got {len(test_corpora)} instead'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def padSentence(sentence, startToken='<s>', stopToken='</s>'):\n",
        "    return [startToken] + sentence + [stopToken]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN ['Man', 'charged', 'over', 'drugs', 'seizure'] -> ['<s>', 'Man', 'charged', 'over', 'drugs', 'seizure', '</s>']\n",
            "TEST ['If', 'you', 'have', 'owned', 'the', 'property', 'for', 'more', 'than', 'three', 'years', ',', 'you', 'can', 'apply', 'for', '\"', 'taper', 'relief', ',', '\"', 'by', 'which', 'you', 'can', 'reduce', 'any', 'taxable', 'gain', 'by', '5%', 'for', 'each', 'year', 'of', 'ownership', ',', 'up', 'to', 'a', 'maximum', '40%', '.'] -> ['<s>', 'The', 'road', 'was', 'pitted', 'with', 'tank', 'treads', '.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "train_padded_corpora = [ padSentence(sentence) for sentence in train_corpora ]\n",
        "print('TRAIN', train_corpora[1], '->', train_padded_corpora[1])\n",
        "\n",
        "test_padded_corpora = [padSentence(sentence) for sentence in test_corpora]\n",
        "print('TEST', test_corpora[1], '->', test_padded_corpora[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Lowercase all words in the training and test corpora. Note that the data already has been tokenized (i.e. the punctuation has been split off words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lowerSentence(sentence):\n",
        "    return [ word.lower() for word in sentence ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN ['<s>', 'Man', 'charged', 'over', 'drugs', 'seizure', '</s>'] -> ['<s>', 'man', 'charged', 'over', 'drugs', 'seizure', '</s>']\n",
            "TEST ['<s>', 'The', 'road', 'was', 'pitted', 'with', 'tank', 'treads', '.', '</s>'] -> ['<s>', 'the', 'road', 'was', 'pitted', 'with', 'tank', 'treads', '.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "train_lowercased_padded_corpora = [ lowerSentence(sentence) for sentence in train_padded_corpora ]\n",
        "print('TRAIN', train_padded_corpora[1], '->', train_lowercased_padded_corpora[1])\n",
        "assert ' '.join(train_lowercased_padded_corpora[1]) == '<s> man charged over drugs seizure </s>', f'train_lowercased_padded_corpora[1] must be \"<s> man charged over drugs seizure </s>\", got {train_lowercased_padded_corpora[1]} instead'\n",
        "train_lowercased_padded_vocabulary = set(word for sentence in train_lowercased_padded_corpora for word in sentence)\n",
        "\n",
        "test_lowercased_padded_corpora = [ lowerSentence(sentence) for sentence in test_padded_corpora ]\n",
        "print('TEST', test_padded_corpora[2], '->', test_lowercased_padded_corpora[2])\n",
        "assert ' '.join(test_lowercased_padded_corpora[2]) == '<s> the road was pitted with tank treads . </s>', f'test_lowercased_padded_corpora[2] must be \"<s> the road was pitted with tank treads . </s>\", got {lowercased_padded_test[1]} instead'\n",
        "lowercased_padded_test_vocabulary = set(word for sentence in test_lowercased_padded_corpora for word in sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Replace all words occurring in the training data once with the token \\<unk>. Everyword in the test data not seen in training should be treated as \\<unk>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import tee\n",
        "\n",
        "def loadSentences(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        corpora = f.read().strip()\n",
        "        return [ sentence.strip().split(' ') for sentence in corpora.split('\\n') ]\n",
        "\n",
        "def padSentence(sentence, startToken='<s>', stopToken='</s>'):\n",
        "    return [startToken] + sentence + [stopToken]\n",
        "\n",
        "def lowerSentence(sentence):\n",
        "    return [ word.lower() for word in sentence ]\n",
        "\n",
        "def pairwise(iterable):\n",
        "    a, b = tee(iterable)\n",
        "    next(b, None)\n",
        "    return zip(a, b)\n",
        "\n",
        "def sentenceToWords(sentence, startToken='<s>', stopToken='</s>', unknownToken='<unk>', unknownWords=set(), knownWords=set(), ignoreWords=set()):\n",
        "    words = sentence.strip().lower().split(' ')\n",
        "    isUnknown = lambda word: word in unknownWords or (len(knownWords) > 0 and word not in knownWords)\n",
        "    words = [ unknownToken if isUnknown(word) else word for word in words ]\n",
        "    words =  [startToken, *words, stopToken]\n",
        "    words = [ word for word in words if word not in ignoreWords ]\n",
        "    return words\n",
        "\n",
        "def replaceWordsInSentence(sentence, wordsForReplacing=[], replacingToken='<unk>'):\n",
        "    return [\n",
        "        word if word not in wordsForReplacing else replacingToken\n",
        "        for word in sentence\n",
        "    ]\n",
        "\n",
        "def filenameToCorpora(filename, startToken='<s>', stopToken='</s>', unknownToken='<unk>', unknownWords=[]):\n",
        "    return [\n",
        "        replaceWordsInSentence(\n",
        "            lowerSentence(\n",
        "                padSentence(sentence, startToken, stopToken)),\n",
        "            wordsForReplacing=unknownWords, replacingToken=unknownToken)\n",
        "        for sentence in loadSentences(filename)\n",
        "    ]\n",
        "\n",
        "def processGrams(corpora):\n",
        "    unigrams = {}\n",
        "    bigrams = {}\n",
        "    for sentence in corpora:\n",
        "        for (condition, word) in pairwise(sentence):\n",
        "            if condition not in bigrams:\n",
        "                bigrams[condition] = {}\n",
        "            if word not in bigrams[condition]:\n",
        "                bigrams[condition][word] = 0\n",
        "            bigrams[condition][word] += 1\n",
        "        for token_type in sentence:\n",
        "            if token_type not in unigrams:\n",
        "                unigrams[token_type] = 0\n",
        "            unigrams[token_type] += 1\n",
        "    return unigrams, bigrams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_corpora = filenameToCorpora('data/train.txt', startToken='<s>', stopToken='</s>')\n",
        "train_unigrams, train_bigrams = processGrams(train_corpora)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total sentences 100000\n",
            "total words 2368210\n",
            "total words with start/stop 2568210\n",
            "total unigrams types 83043\n",
            "total unigrams types with start/stop 83045\n",
            "total unigrams words 2368210\n",
            "total unigrams words with start/stop 2568210\n",
            "total bigrams types with start/stop 83044\n",
            "total bigrams words with start/stop 809973\n"
          ]
        }
      ],
      "source": [
        "print('total sentences', len(train_corpora))\n",
        "\n",
        "print('total words', sum(len([word for word in sentence if word not in {'<s>', '</s>'}]) for sentence in train_corpora))\n",
        "print('total words with start/stop', sum(count for count  in train_unigrams.values()))\n",
        "\n",
        "print('total unigrams types', len([ count for unigram, count in train_unigrams.items() if unigram not in {'<s>', '</s>'} ]))\n",
        "print('total unigrams types with start/stop', len(train_unigrams))\n",
        "\n",
        "print('total unigrams words', sum(count for unigram, count in train_unigrams.items() if unigram not in {'<s>', '</s>'}))\n",
        "print('total unigrams words with start/stop', sum(train_unigrams.values()))\n",
        "\n",
        "print('total bigrams types with start/stop', len(train_bigrams))\n",
        "print('total bigrams words with start/stop', sum(len(condition_word.values()) for condition_word in train_bigrams.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_corpora = filenameToCorpora('data/test.txt', startToken='<s>', stopToken='</s>')\n",
        "test_unigrams, test_bigrams = processGrams(test_corpora)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total sentences 100\n",
            "total words 2669\n",
            "total words with start/stop 2869\n",
            "total unigrams types 1247\n",
            "total unigrams types with start/stop 1249\n",
            "total unigrams words 2669\n",
            "total unigrams words with start/stop 2869\n",
            "total bigrams types with start/stop 1248\n",
            "total bigrams words with start/stop 2421\n"
          ]
        }
      ],
      "source": [
        "print('total sentences', len(test_corpora))\n",
        "\n",
        "print('total words', sum(len([word for word in sentence if word not in {'<s>', '</s>'}]) for sentence in test_corpora))\n",
        "print('total words with start/stop', sum(count for count  in test_unigrams.values()))\n",
        "\n",
        "print('total unigrams types', len([ count for unigram, count in test_unigrams.items() if unigram not in {'<s>', '</s>'} ]))\n",
        "print('total unigrams types with start/stop', len(test_unigrams))\n",
        "\n",
        "print('total unigrams words', sum(count for unigram, count in test_unigrams.items() if unigram not in {'<s>', '</s>'}))\n",
        "print('total unigrams words with start/stop', sum(test_unigrams.values()))\n",
        "\n",
        "print('total bigrams types with start/stop', len(test_bigrams))\n",
        "print('total bigrams words with start/stop', sum(len(condition_word.values()) for condition_word in test_bigrams.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_only_unigrams = { unigram: count for unigram, count in test_unigrams.items() if unigram not in train_unigrams }\n",
        "\n",
        "test_only_bigrams = {}\n",
        "for condition, word_count in test_bigrams.items():\n",
        "    if condition not in train_bigrams:\n",
        "        test_only_bigrams[condition] = word_count\n",
        "    else:\n",
        "        for word, count in word_count.items():\n",
        "            if word not in train_bigrams[condition]:\n",
        "                if condition not in test_only_bigrams:\n",
        "                    test_only_bigrams[condition] = {}\n",
        "                test_only_bigrams[condition][word] = count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "only test unigrams types 45\n",
            "only test unigrams words 46\n",
            "only test bigram types 722\n",
            "only test bigram words 725\n"
          ]
        }
      ],
      "source": [
        "print('only test unigrams types', len(test_only_unigrams.keys()))\n",
        "print('only test unigrams words', sum(test_only_unigrams.values()))\n",
        "\n",
        "print('only test bigram types', sum(len(token_count.values()) for token_count in test_only_bigrams.values()))\n",
        "print('only test bigram words', sum(sum(token_count.values()) for token_count in test_only_bigrams.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_once_unigrams = { word: count for word, count in train_unigrams.items() if count == 1 }\n",
        "\n",
        "train_corpora_with_replacing = filenameToCorpora('data/train.txt', startToken='<s>', stopToken='</s>', unknownToken='<unk>', unknownWords=set(train_once_unigrams.keys()))\n",
        "train_unigrams_with_replacing, train_bigrams_with_replacing = processGrams(train_corpora_with_replacing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total sentences 100000\n",
            "total words 2368210\n",
            "total words with start/stop 2568210\n",
            "total unigrams types 41737\n",
            "total unigrams types with start/stop 41739\n",
            "total unigrams words 2368210\n",
            "total unigrams words with start/stop 2568210\n",
            "total bigrams types with start/stop 41738\n",
            "total bigrams words with start/stop 742294\n"
          ]
        }
      ],
      "source": [
        "print('total sentences', len(train_corpora_with_replacing))\n",
        "\n",
        "print('total words', sum(len([word for word in sentence if word not in {'<s>', '</s>'}]) for sentence in train_corpora_with_replacing))\n",
        "print('total words with start/stop', sum(count for count  in train_unigrams_with_replacing.values()))\n",
        "\n",
        "print('total unigrams types', len([ count for unigram, count in train_unigrams_with_replacing.items() if unigram not in {'<s>', '</s>'} ]))\n",
        "print('total unigrams types with start/stop', len(train_unigrams_with_replacing))\n",
        "\n",
        "print('total unigrams words', sum(count for unigram, count in train_unigrams_with_replacing.items() if unigram not in {'<s>', '</s>'}))\n",
        "print('total unigrams words with start/stop', sum(train_unigrams_with_replacing.values()))\n",
        "\n",
        "print('total bigrams types with start/stop', len(train_bigrams_with_replacing))\n",
        "print('total bigrams words with start/stop', sum(len(condition_word.values()) for condition_word in train_bigrams_with_replacing.values()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "1175\n",
            "2869\n",
            "2365\n",
            "2769\n"
          ]
        }
      ],
      "source": [
        "train_once_test_only_unigrams = set(train_once_unigrams.keys()).union(set(test_only_unigrams.keys()))\n",
        "\n",
        "test_corpora_with_replacing = filenameToCorpora('data/test.txt', startToken='<s>', stopToken='</s>', unknownToken='<unk>', unknownWords=train_once_test_only_unigrams)\n",
        "test_unigrams_with_replacing, test_bigrams_with_replacing = processGrams(test_corpora_with_replacing)\n",
        "print(len(test_corpora_with_replacing))\n",
        "print(len(test_unigrams_with_replacing))\n",
        "print(sum(test_unigrams_with_replacing.values()))\n",
        "print(sum(len(condition_word.values()) for condition_word in test_bigrams_with_replacing.values()))\n",
        "print(sum(sum(condition_word.values()) for condition_word in test_bigrams_with_replacing.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 TRAINING THE MODELS\n",
        "\n",
        "Please use train.txt to train the following language models:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. A unigram maximum likelihood model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class UnigramModel:\n",
        "    def __init__(self, unigrams, ignoredWords={'<s>'}):\n",
        "        self.ignoredWords = ignoredWords\n",
        "        self.unigrams = { unigram: count for unigram, count in unigrams.items() if unigram not in self.ignoredWords }\n",
        "        self.unigrams_count = sum(self.unigrams.values())\n",
        "    \n",
        "    def operatorSumOrProd(self, values, log=False):\n",
        "        return sum(values) if log else math.prod(values)\n",
        "    \n",
        "    def operatorPlusOrTimes(self, log=False):\n",
        "        return ' + ' if log else ' \\\\times '\n",
        "    \n",
        "    def unigramMLE(self, word, log=False, verbose=False):\n",
        "        denominator = self.unigrams_count\n",
        "        if word in self.unigrams:\n",
        "            numerator = self.unigrams[word]\n",
        "            wordProbability = numerator / denominator\n",
        "        else:\n",
        "            numerator = 0\n",
        "            wordProbability = 0\n",
        "        if log:\n",
        "            wordProbability = math.log(wordProbability, 2) if wordProbability > 0 else -math.inf\n",
        "        \n",
        "        steps = [\n",
        "            'P(\\\\texttt{'+ word + '})',\n",
        "            '\\\\frac{count(\\\\texttt{'+ word + '})}{count()}',\n",
        "            '\\\\frac{' + str(numerator) +  '}{' + str(denominator) + '}'\n",
        "        ]\n",
        "        if log:\n",
        "            steps = [ '\\log_{2} (' + step + ')' for step in steps ]\n",
        "        steps.append(wordProbability)\n",
        "\n",
        "        if verbose:\n",
        "            print(steps[0] + ' =&\\\\ ', ' = '.join([ str(step) for step in steps[1:] ]), ' \\\\\\\\')\n",
        "        return steps\n",
        "\n",
        "    def sentenceMLE(self, sentence, log=False, verbose=False):\n",
        "        if verbose:\n",
        "            print(\"\\\\begin{equation}\\\\begin{split}\")\n",
        "            print('S =&\\\\ \\\\texttt{' + sentence + '} \\\\\\\\')\n",
        "\n",
        "        words = sentenceToWords(sentence, knownWords=set(self.unigrams.keys()), ignoreWords=self.ignoredWords)\n",
        "        wordProbabilities = [ self.unigramMLE(word, log=log, verbose=verbose) for word in words if word not in self.ignoredWords ]\n",
        "        sentenceProbability = self.operatorSumOrProd([ wordProbability for *_, wordProbability in wordProbabilities ], log=log)\n",
        "        \n",
        "        steps = [\n",
        "            'P(S)',\n",
        "            'P(\\\\texttt{'+ sentence + '})',\n",
        "            'P(' + ', '.join([ '\\\\texttt{' + word + '}' for word in words]) +')'\n",
        "        ]\n",
        "        if log:\n",
        "            steps = [ '\\log_{2} (' + step + ')' for step in steps ]\n",
        "        steps.append(self.operatorPlusOrTimes(log).join([ wordProbabilityKey for wordProbabilityKey, *_ in wordProbabilities ]))\n",
        "        steps.append(self.operatorPlusOrTimes(log).join([ str(wordProbability) for *_, wordProbability in wordProbabilities ]))\n",
        "        steps.append(len(wordProbabilities))\n",
        "        steps.append(sentenceProbability)\n",
        "\n",
        "        if verbose:\n",
        "            print(steps[0], '=&\\\\', ' =&\\\\ '.join([f'{step} \\\\\\\\' for step in steps[1:]]))\n",
        "            print(\"\\\\end{split}\\\\end{equation}\")\n",
        "        return steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. A bigram maximum likelihood model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class BigramModel(UnigramModel):\n",
        "    def __init__(self, unigrams, bigrams, ignoreWords={}):\n",
        "        super().__init__(unigrams, ignoredWords=ignoreWords)\n",
        "        self.bigrams = bigrams\n",
        "    \n",
        "    def bigramMLE(self, word, condition, log=False, verbose=False):\n",
        "        if condition in self.bigrams and word in self.bigrams[condition]:\n",
        "            bigramCount = self.bigrams[condition][word]\n",
        "            unigramCount = self.unigrams[condition]\n",
        "            conditionalProbability = bigramCount / unigramCount\n",
        "        else:\n",
        "            bigramCount = 0\n",
        "            unigramCount = 0\n",
        "            conditionalProbability = 0\n",
        "        if log:\n",
        "            conditionalProbability = math.log(conditionalProbability, 2) if conditionalProbability > 0 else -math.inf\n",
        "        steps = [\n",
        "            'P(\\\\texttt{' + condition + '} \\mid \\\\texttt{' + word + '})',\n",
        "            '\\\\frac{count(\\\\texttt{' + condition + '} , \\\\texttt{' + word + '})}{count(' + condition + ')}',\n",
        "            '\\\\frac{' + str(bigramCount) +  '}{' + str(unigramCount) + '}'\n",
        "        ]\n",
        "        if log:\n",
        "            steps = [ '\\log_{2} (' + step + ')' for step in steps ]\n",
        "        steps.append(conditionalProbability)\n",
        "        if verbose:\n",
        "            print(steps[0] + ' =&\\\\ ', ' = '.join([ str(step) for step in steps[1:] ]), ' \\\\\\\\')\n",
        "        return steps\n",
        "\n",
        "    def sentenceMLE(self, sentence, log=False, verbose=False):\n",
        "        if verbose:\n",
        "            print(\"\\\\begin{equation}\\\\begin{split}\")\n",
        "            print('S =&\\\\ \\\\texttt{' + sentence + '} \\\\\\\\')\n",
        "        \n",
        "        words = sentenceToWords(sentence, knownWords=set(self.unigrams.keys()), ignoreWords=self.ignoredWords)\n",
        "\n",
        "        wordProbabilities = [ self.bigramMLE(word, condition, log=log, verbose=verbose) for condition, word in pairwise(words) ]\n",
        "        sentenceProbability = self.operatorSumOrProd([ wordProbability for *_, wordProbability in wordProbabilities ], log=log)\n",
        "\n",
        "        steps = [\n",
        "            'P(S)',\n",
        "            'P(\\\\texttt{'+ sentence + '})',\n",
        "            'P(' + ', '.join([ '\\\\texttt{' + word + '}' for word in words]) +')'\n",
        "        ]\n",
        "        if log:\n",
        "            steps = [ '\\log_{2} (' + step + ')' for step in steps ]\n",
        "        steps.append(self.operatorPlusOrTimes(log).join([ wordProbabilityKey for wordProbabilityKey, *_ in wordProbabilities ]))\n",
        "        steps.append(self.operatorPlusOrTimes(log).join([ str(wordProbability) for *_, wordProbability in wordProbabilities ]))\n",
        "        steps.append(len(wordProbabilities))\n",
        "        steps.append(sentenceProbability)\n",
        "\n",
        "        if verbose:\n",
        "            print(steps[0], '&=', ' =&\\\\ '.join([f'{step} \\\\\\\\' for step in steps[1:]]))\n",
        "            print(\"\\\\end{split}\\\\end{equation}\")\n",
        "        return steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. A bigram model with Add-One smoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class SmoothBigramModel(BigramModel):\n",
        "    def bigramMLE(self, word, condition, log=False, verbose=False):\n",
        "        if condition in self.bigrams and word in self.bigrams[condition]:\n",
        "            bigramCount = self.bigrams[condition][word]\n",
        "        else:\n",
        "            bigramCount = 0\n",
        "        if condition in self.unigrams:\n",
        "            unigramCount = self.unigrams[condition]\n",
        "        else:\n",
        "            unigramCount = 0\n",
        "        vocab_size = len(self.unigrams.keys())\n",
        "        conditionalProbability = (bigramCount + 1) / (unigramCount + vocab_size)\n",
        "        if log:\n",
        "            conditionalProbability = math.log(conditionalProbability, 2) if conditionalProbability > 0 else -math.inf\n",
        "        steps = [\n",
        "            'P(\\\\texttt{' + condition + '} \\mid \\\\texttt{' + word + '})',\n",
        "            '\\\\frac{count^{*}(\\\\texttt{' + condition + '} , \\\\texttt{' + word + '}) + 1}{count(' + condition + ') + |V|}',\n",
        "            '\\\\frac{' + str(bigramCount) +  ' + 1}{' + str(unigramCount) + ' + ' + str(vocab_size) + '}'\n",
        "        ]\n",
        "        if log:\n",
        "            steps = [ '\\log_{2} (' + step + ')' for step in steps ]\n",
        "        steps.append(conditionalProbability)\n",
        "        if verbose:\n",
        "            print(steps[0] + ' =&\\\\ ', ' = '.join([ str(step) for step in steps[1:] ]), ' \\\\\\\\')\n",
        "        return steps\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. A bigram model with discounting and Katz backoff. Please use a discount constant of 0.5 (see lecture on smoothing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KatzBigramModel(BigramModel):\n",
        "    def __init__(self, unigrams, bigrams, ignoreWords={}):\n",
        "        BigramModel.__init__(self, unigrams, bigrams, ignoreWords=ignoreWords)\n",
        "\n",
        "        self.bigrams_star = {}\n",
        "        for condition, word_count in self.bigrams.items():\n",
        "            if condition not in self.bigrams_star:\n",
        "                self.bigrams_star[condition] = {}\n",
        "            for word, count in word_count.items():\n",
        "                if word not in self.bigrams_star[condition]:\n",
        "                    self.bigrams_star[condition][word] = 0\n",
        "                self.bigrams_star[condition][word] = count - 0.5\n",
        "        self.A = {}\n",
        "        self.B = {}\n",
        "        self.P_B = {}\n",
        "        self.P_ML = { word: self.unigramMLE(word)[-1] for word in self.unigrams }\n",
        "    \n",
        "    def trainB(self, condition):\n",
        "        self.A[condition] = {}\n",
        "        self.B[condition] = {}\n",
        "        for unigram in self.unigrams:\n",
        "            if condition in self.bigrams and unigram in self.bigrams[condition]:\n",
        "                self.A[condition][unigram] = self.bigrams[condition][unigram]\n",
        "        for unigram, count in self.unigrams.items():\n",
        "            if unigram in self.A[condition]:\n",
        "                continue\n",
        "            self.B[condition][unigram] = count\n",
        "        self.P_B[condition] = sum(self.P_ML[word] for word in self.B[condition])\n",
        "\n",
        "    def trainBs(self, bigrams):\n",
        "        for condition, unigrams in bigrams.items():\n",
        "            for word in unigrams:\n",
        "                if condition in self.bigrams and word in self.bigrams[condition]:\n",
        "                    continue\n",
        "                self.trainB(condition)\n",
        "\n",
        "    def bigramMLE(self, word, condition, log=False, verbose=False):\n",
        "        alpha_steps = None\n",
        "        steps = ['P(\\\\texttt{' + condition + '} \\mid \\\\texttt{' + word + '})']\n",
        "        if condition in self.bigrams_star and word in self.bigrams_star[condition]:\n",
        "            bigramCount = self.bigrams_star[condition][word]\n",
        "            unigramCount = self.unigrams[condition]\n",
        "            conditionalProbability = bigramCount / unigramCount\n",
        "            steps.append('\\\\frac{count^{*}(\\\\texttt{' + condition + '} , \\\\texttt{' + word + '})}{count(\\\\texttt{' + condition + '})}')\n",
        "            steps.append('\\\\frac{' + str(bigramCount) +  '}{' + str(unigramCount) + '}')\n",
        "        else:\n",
        "            if condition not in self.P_B:\n",
        "                self.trainB(condition)\n",
        "            \n",
        "            alpha_numerator = sum(self.bigrams_star[condition].values())\n",
        "            alpha_denominator = self.unigrams[condition]\n",
        "            alpha_condition = 1 - alpha_numerator / alpha_denominator\n",
        "            alpha_steps = [\n",
        "                '\\\\alpha_{\\\\texttt{' + condition + '}}',\n",
        "                '1 - \\\\frac{\\\\Sigma_{w} count^{*}(\\\\texttt{' + condition + '} , \\\\texttt{' + word + '})}{count(\\\\texttt{' + condition + '})}',\n",
        "                '1 - \\\\frac{' + str(alpha_numerator) +  '}{' + str(alpha_denominator) + '}',\n",
        "                alpha_condition\n",
        "            ]\n",
        "            *_, wordProbability = self.unigramMLE(word, log=False)\n",
        "            wordProbabilitiesSum = self.P_B[condition]\n",
        "            conditionalProbability = alpha_condition * wordProbability / wordProbabilitiesSum\n",
        "            steps.append('\\\\alpha_{\\\\texttt{' + condition + '}} \\\\times \\\\frac{P_{ML}(\\\\texttt{' + word + '})}{\\\\Sigma_{w \\\\in B_{\\\\texttt{' + condition + '}}} P(\\\\texttt{w})}')\n",
        "            steps.append(str(alpha_condition) + '\\\\times \\\\frac{' + str(wordProbability) +  '}{' + str(wordProbabilitiesSum) + '}')\n",
        "        \n",
        "        if log:\n",
        "            conditionalProbability = math.log(conditionalProbability, 2) if conditionalProbability > 0 else -math.inf\n",
        "        if log:\n",
        "            steps = [ '\\log_{2} (' + step + ')' for step in steps ]\n",
        "        steps.append(conditionalProbability)\n",
        "        if verbose:\n",
        "            if alpha_steps:\n",
        "                print(alpha_steps[0] + ' =&\\\\ ', ' = '.join([ str(step) for step in alpha_steps[1:] ]), ' \\\\\\\\')\n",
        "            print(steps[0] + ' =&\\\\ ', ' = '.join([ str(step) for step in steps[1:] ]), ' \\\\\\\\')\n",
        "        return steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 QUESTIONS\n",
        "\n",
        "Please answer the questions below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. (5 points) How many word types (unique words) are there in the training corpus? Please include the end-of-sentence padding symbol \\</s> and the unknown token \\<unk>. Do not include the start of sentence padding symbol \\<s>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 41738 word types (unique words) in the training corpus.\n"
          ]
        }
      ],
      "source": [
        "excluded_unigrams =  {'<s>'}\n",
        "train_unigrams_with_replacing_without_start = set(train_unigrams_with_replacing.keys()) - excluded_unigrams\n",
        "print(f'There are {len(train_unigrams_with_replacing_without_start)} word types (unique words) in the training corpus.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. (5 points) How many word tokens are there in the training corpus? Do not include the start of sentence padding symbol \\<s>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 2468210 word tokens in the training corpus.\n"
          ]
        }
      ],
      "source": [
        "excluded_unigrams =  {'<s>'}\n",
        "\n",
        "train_unigrams_with_replacing_without_start = sum([ value for word, value in train_unigrams_with_replacing.items() if word not in excluded_unigrams ])\n",
        "print(f'There are {train_unigrams_with_replacing_without_start} word tokens in the training corpus.')\n",
        "\n",
        "# test_unigrams_with_replacing_without_start = sum([ value for word, value in test_unigrams_with_replacing.items() if word not in {'<s>'} ])\n",
        "# print('test unigrams words count', test_unigrams_with_replacing_without_start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. (10 points) What percentage of word tokens and word types in the test corpus did not occur in training (before you mapped the unknown words to \\<unk> in training and test data)? Please include the padding symbol \\</s> in your calculations. Do not include the start of sentence padding symbol \\<s>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\begin{itemize}\n",
            "\\item Word tokens only in test corpus: 46\n",
            "\\item Word tokens in test corpus 2769\n",
            "\\item Percentage of word tokens only in test corpus: 1.6612495485734922\n",
            "\\item Word types only in test corpus: 45\n",
            "\\item Word types in test corpus: 1248\n",
            "\\item Percentage of word types only in test corpus: 3.6057692307692304\n",
            "\\end{itemize}\n"
          ]
        }
      ],
      "source": [
        "excluded_unigrams =  {'<s>'}\n",
        "test_only_unigrams_without_start = { word: count for word, count in test_only_unigrams.items() if word not in excluded_unigrams }\n",
        "test_unigrams_without_start = { word: count for word, count in test_unigrams.items() if word not in excluded_unigrams }\n",
        "\n",
        "print('\\\\begin{itemize}')\n",
        "print('\\\\item Word tokens only in test corpus:', sum(test_only_unigrams_without_start.values()))\n",
        "print('\\\\item Word tokens in test corpus', sum(test_unigrams_without_start.values()))\n",
        "print('\\\\item Percentage of word tokens only in test corpus:', sum(test_only_unigrams_without_start.values()) / sum(test_unigrams_without_start.values()) * 100)\n",
        "\n",
        "print('\\\\item Word types only in test corpus:', len(test_only_unigrams_without_start))\n",
        "print('\\\\item Word types in test corpus:', len(test_unigrams_without_start))\n",
        "print('\\\\item Percentage of word types only in test corpus:', len(test_only_unigrams_without_start) / len(test_unigrams_without_start) * 100)\n",
        "print('\\\\end{itemize}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. (15 points) Now replace singletons in the training data with \\<unk> symbol and map words (in the test corpus) not observed in training to \\<unk>. What percentage of bigrams (bigram types and bigram tokens) in the test corpus did not occur in training (treat \\<unk> as a regular token that has been observed). Please include the padding symbol \\</s> in your calculations. Do not include the start of sentence padding symbol \\<s>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\begin{itemize}\n",
            "\\item Bigrams types only in test corpus: 595\n",
            "\\item Bigrams types in test corpus: 2300\n",
            "\\item \\textbf{Percentage of bigrams types only in test corpus}: 25.869565217391305\n",
            "\\item Bigrams tokens only in test corpus: 597\n",
            "\\item Bigrams tokens in test corpus: 2669\n",
            "\\item \\textbf{Percentage of bigrams tokens only in test}: 22.367928062944923\n",
            "\\end{itemize}\n"
          ]
        }
      ],
      "source": [
        "excluded_unigrams =  {'<s>'}\n",
        "\n",
        "test_only_bigrams_with_replacing_no_start = {}\n",
        "for condition, words in test_bigrams_with_replacing.items():\n",
        "    if condition not in excluded_unigrams:\n",
        "        if condition not in train_bigrams_with_replacing:\n",
        "            for word, count in words.items():\n",
        "                test_only_bigrams_with_replacing_no_start[(condition, word)] = count\n",
        "        else:\n",
        "            for word, count in words.items():\n",
        "                if word not in train_bigrams_with_replacing[condition]:\n",
        "                    test_only_bigrams_with_replacing_no_start[(condition, word)] = count\n",
        "\n",
        "test_bigrams_with_replacing_no_start = {}\n",
        "for condition, words in test_bigrams_with_replacing.items():\n",
        "    if condition not in excluded_unigrams:\n",
        "        for word, count in words.items():\n",
        "            test_bigrams_with_replacing_no_start[(condition, word)] = count\n",
        "\n",
        "test_only_bigrams_types_with_replacing_no_start_count = len(test_only_bigrams_with_replacing_no_start)\n",
        "test_only_bigrams_words_with_replacing_no_start_count = sum(test_only_bigrams_with_replacing_no_start.values())\n",
        "test_bigrams_types_with_replacing_no_start_count = len(test_bigrams_with_replacing_no_start)\n",
        "test_bigrams_words_with_replacing_no_start_count = sum(test_bigrams_with_replacing_no_start.values())\n",
        "\n",
        "print('\\\\begin{itemize}')\n",
        "print('\\\\item Bigrams types only in test corpus:', test_only_bigrams_types_with_replacing_no_start_count)\n",
        "print('\\\\item Bigrams types in test corpus:', test_bigrams_types_with_replacing_no_start_count)\n",
        "print('\\\\item \\\\textbf{Percentage of bigrams types only in test corpus}:', test_only_bigrams_types_with_replacing_no_start_count / test_bigrams_types_with_replacing_no_start_count * 100)\n",
        "\n",
        "print('\\\\item Bigrams tokens only in test corpus:', test_only_bigrams_words_with_replacing_no_start_count)\n",
        "print('\\\\item Bigrams tokens in test corpus:', test_bigrams_words_with_replacing_no_start_count)\n",
        "print('\\\\item \\\\textbf{Percentage of bigrams tokens only in test}:', test_only_bigrams_words_with_replacing_no_start_count / test_bigrams_words_with_replacing_no_start_count * 100)\n",
        "print('\\\\end{itemize}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. (15 points) Compute the log probability of the following sentence under the three models (ignore capitalization and pad each sentence as described above). Please list all of the parameters required to compute the probabilities and show the complete calculation. Which of the parameters have zero values under each model? Use log base 2 in your calculations. Map words not observed in the training corpus to the \\<unk> token.\n",
        "\n",
        "- I look forward to hearing your reply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\begin{equation}\\begin{split}\n",
            "S =&\\ \\texttt{I look forward to hearing your reply .} \\\\\n",
            "\\log_{2} (P(\\texttt{i})) =&\\  \\log_{2} (\\frac{count(\\texttt{i})}{count()}) = \\log_{2} (\\frac{7339}{2468210}) = -8.39366593438855  \\\\\n",
            "\\log_{2} (P(\\texttt{look})) =&\\  \\log_{2} (\\frac{count(\\texttt{look})}{count()}) = \\log_{2} (\\frac{613}{2468210}) = -11.97529045258011  \\\\\n",
            "\\log_{2} (P(\\texttt{forward})) =&\\  \\log_{2} (\\frac{count(\\texttt{forward})}{count()}) = \\log_{2} (\\frac{474}{2468210}) = -12.346290467372631  \\\\\n",
            "\\log_{2} (P(\\texttt{to})) =&\\  \\log_{2} (\\frac{count(\\texttt{to})}{count()}) = \\log_{2} (\\frac{53048}{2468210}) = -5.540022976617652  \\\\\n",
            "\\log_{2} (P(\\texttt{hearing})) =&\\  \\log_{2} (\\frac{count(\\texttt{hearing})}{count()}) = \\log_{2} (\\frac{209}{2468210}) = -13.527674584190008  \\\\\n",
            "\\log_{2} (P(\\texttt{your})) =&\\  \\log_{2} (\\frac{count(\\texttt{your})}{count()}) = \\log_{2} (\\frac{1217}{2468210}) = -10.985920263557162  \\\\\n",
            "\\log_{2} (P(\\texttt{reply})) =&\\  \\log_{2} (\\frac{count(\\texttt{reply})}{count()}) = \\log_{2} (\\frac{13}{2468210}) = -17.5345939981298  \\\\\n",
            "\\log_{2} (P(\\texttt{.})) =&\\  \\log_{2} (\\frac{count(\\texttt{.})}{count()}) = \\log_{2} (\\frac{87894}{2468210}) = -4.811556652191113  \\\\\n",
            "\\log_{2} (P(\\texttt{</s>})) =&\\  \\log_{2} (\\frac{count(\\texttt{</s>})}{count()}) = \\log_{2} (\\frac{100000}{2468210}) = -4.625393241834078  \\\\\n",
            "\\log_{2} (P(S)) =&\\ \\log_{2} (P(\\texttt{I look forward to hearing your reply .})) \\\\ =&\\ \\log_{2} (P(\\texttt{i}, \\texttt{look}, \\texttt{forward}, \\texttt{to}, \\texttt{hearing}, \\texttt{your}, \\texttt{reply}, \\texttt{.}, \\texttt{</s>})) \\\\ =&\\ \\log_{2} (P(\\texttt{i})) + \\log_{2} (P(\\texttt{look})) + \\log_{2} (P(\\texttt{forward})) + \\log_{2} (P(\\texttt{to})) + \\log_{2} (P(\\texttt{hearing})) + \\log_{2} (P(\\texttt{your})) + \\log_{2} (P(\\texttt{reply})) + \\log_{2} (P(\\texttt{.})) + \\log_{2} (P(\\texttt{</s>})) \\\\ =&\\ -8.39366593438855 + -11.97529045258011 + -12.346290467372631 + -5.540022976617652 + -13.527674584190008 + -10.985920263557162 + -17.5345939981298 + -4.811556652191113 + -4.625393241834078 \\\\ =&\\ 9 \\\\ =&\\ -89.74040857086109 \\\\\n",
            "\\end{split}\\end{equation}\n",
            "\\begin{equation}\\begin{split}\n",
            "S =&\\ \\texttt{I look forward to hearing your reply .} \\\\\n",
            "P(\\texttt{i}) =&\\  \\frac{count(\\texttt{i})}{count()} = \\frac{7339}{2468210} = 0.002973409880034519  \\\\\n",
            "P(\\texttt{look}) =&\\  \\frac{count(\\texttt{look})}{count()} = \\frac{613}{2468210} = 0.00024835812187779807  \\\\\n",
            "P(\\texttt{forward}) =&\\  \\frac{count(\\texttt{forward})}{count()} = \\frac{474}{2468210} = 0.000192042006150206  \\\\\n",
            "P(\\texttt{to}) =&\\  \\frac{count(\\texttt{to})}{count()} = \\frac{53048}{2468210} = 0.021492498612354704  \\\\\n",
            "P(\\texttt{hearing}) =&\\  \\frac{count(\\texttt{hearing})}{count()} = \\frac{209}{2468210} = 8.467674954724274e-05  \\\\\n",
            "P(\\texttt{your}) =&\\  \\frac{count(\\texttt{your})}{count()} = \\frac{1217}{2468210} = 0.0004930698765502125  \\\\\n",
            "P(\\texttt{reply}) =&\\  \\frac{count(\\texttt{reply})}{count()} = \\frac{13}{2468210} = 5.2669748522208405e-06  \\\\\n",
            "P(\\texttt{.}) =&\\  \\frac{count(\\texttt{.})}{count()} = \\frac{87894}{2468210} = 0.03561042212777681  \\\\\n",
            "P(\\texttt{</s>}) =&\\  \\frac{count(\\texttt{</s>})}{count()} = \\frac{100000}{2468210} = 0.04051519117092954  \\\\\n",
            "P(S) =&\\ P(\\texttt{I look forward to hearing your reply .}) \\\\ =&\\ P(\\texttt{i}, \\texttt{look}, \\texttt{forward}, \\texttt{to}, \\texttt{hearing}, \\texttt{your}, \\texttt{reply}, \\texttt{.}, \\texttt{</s>}) \\\\ =&\\ P(\\texttt{i}) \\times P(\\texttt{look}) \\times P(\\texttt{forward}) \\times P(\\texttt{to}) \\times P(\\texttt{hearing}) \\times P(\\texttt{your}) \\times P(\\texttt{reply}) \\times P(\\texttt{.}) \\times P(\\texttt{</s>}) \\\\ =&\\ 0.002973409880034519 \\times 0.00024835812187779807 \\times 0.000192042006150206 \\times 0.021492498612354704 \\times 8.467674954724274e-05 \\times 0.0004930698765502125 \\times 5.2669748522208405e-06 \\times 0.03561042212777681 \\times 0.04051519117092954 \\\\ =&\\ 9 \\\\ =&\\ 9.670416894079104e-28 \\\\\n",
            "\\end{split}\\end{equation}\n"
          ]
        }
      ],
      "source": [
        "unigramModel = UnigramModel(train_unigrams_with_replacing, ignoredWords={'<s>'})\n",
        "unigramLogOutput = unigramModel.sentenceMLE('I look forward to hearing your reply .', verbose=True, log=True)\n",
        "unigramOutput = unigramModel.sentenceMLE('I look forward to hearing your reply .', verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\begin{equation}\\begin{split}\n",
            "S =&\\ \\texttt{I look forward to hearing your reply .} \\\\\n",
            "\\log_{2} (P(\\texttt{<s>} \\mid \\texttt{i})) =&\\  \\log_{2} (\\frac{count(\\texttt{<s>} , \\texttt{i})}{count(<s>)}) = \\log_{2} (\\frac{2006}{100000}) = -5.639534583824631  \\\\\n",
            "\\log_{2} (P(\\texttt{i} \\mid \\texttt{look})) =&\\  \\log_{2} (\\frac{count(\\texttt{i} , \\texttt{look})}{count(i)}) = \\log_{2} (\\frac{15}{7339}) = -8.93447718627382  \\\\\n",
            "\\log_{2} (P(\\texttt{look} \\mid \\texttt{forward})) =&\\  \\log_{2} (\\frac{count(\\texttt{look} , \\texttt{forward})}{count(look)}) = \\log_{2} (\\frac{34}{613}) = -4.172280422440442  \\\\\n",
            "\\log_{2} (P(\\texttt{forward} \\mid \\texttt{to})) =&\\  \\log_{2} (\\frac{count(\\texttt{forward} , \\texttt{to})}{count(forward)}) = \\log_{2} (\\frac{100}{474}) = -2.2448870591235344  \\\\\n",
            "\\log_{2} (P(\\texttt{to} \\mid \\texttt{hearing})) =&\\  \\log_{2} (\\frac{count(\\texttt{to} , \\texttt{hearing})}{count(to)}) = \\log_{2} (\\frac{6}{53048}) = -13.110048238932082  \\\\\n",
            "\\log_{2} (P(\\texttt{hearing} \\mid \\texttt{your})) =&\\  \\log_{2} (\\frac{count(\\texttt{hearing} , \\texttt{your})}{count(hearing)}) = \\log_{2} (\\frac{0}{0}) = -inf  \\\\\n",
            "\\log_{2} (P(\\texttt{your} \\mid \\texttt{reply})) =&\\  \\log_{2} (\\frac{count(\\texttt{your} , \\texttt{reply})}{count(your)}) = \\log_{2} (\\frac{0}{0}) = -inf  \\\\\n",
            "\\log_{2} (P(\\texttt{reply} \\mid \\texttt{.})) =&\\  \\log_{2} (\\frac{count(\\texttt{reply} , \\texttt{.})}{count(reply)}) = \\log_{2} (\\frac{0}{0}) = -inf  \\\\\n",
            "\\log_{2} (P(\\texttt{.} \\mid \\texttt{</s>})) =&\\  \\log_{2} (\\frac{count(\\texttt{.} , \\texttt{</s>})}{count(.)}) = \\log_{2} (\\frac{82888}{87894}) = -0.08460143194821208  \\\\\n",
            "\\log_{2} (P(S)) &= \\log_{2} (P(\\texttt{I look forward to hearing your reply .})) \\\\ =&\\ \\log_{2} (P(\\texttt{<s>}, \\texttt{i}, \\texttt{look}, \\texttt{forward}, \\texttt{to}, \\texttt{hearing}, \\texttt{your}, \\texttt{reply}, \\texttt{.}, \\texttt{</s>})) \\\\ =&\\ \\log_{2} (P(\\texttt{<s>} \\mid \\texttt{i})) + \\log_{2} (P(\\texttt{i} \\mid \\texttt{look})) + \\log_{2} (P(\\texttt{look} \\mid \\texttt{forward})) + \\log_{2} (P(\\texttt{forward} \\mid \\texttt{to})) + \\log_{2} (P(\\texttt{to} \\mid \\texttt{hearing})) + \\log_{2} (P(\\texttt{hearing} \\mid \\texttt{your})) + \\log_{2} (P(\\texttt{your} \\mid \\texttt{reply})) + \\log_{2} (P(\\texttt{reply} \\mid \\texttt{.})) + \\log_{2} (P(\\texttt{.} \\mid \\texttt{</s>})) \\\\ =&\\ -5.639534583824631 + -8.93447718627382 + -4.172280422440442 + -2.2448870591235344 + -13.110048238932082 + -inf + -inf + -inf + -0.08460143194821208 \\\\ =&\\ 9 \\\\ =&\\ -inf \\\\\n",
            "\\end{split}\\end{equation}\n"
          ]
        }
      ],
      "source": [
        "bigramModel = BigramModel(train_unigrams_with_replacing, train_bigrams_with_replacing, ignoreWords={})\n",
        "bigramLogOutput = bigramModel.sentenceMLE('I look forward to hearing your reply .', verbose=True, log=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\begin{equation}\\begin{split}\n",
            "S =&\\ \\texttt{I look forward to hearing your reply .} \\\\\n",
            "\\log_{2} (P(\\texttt{<s>} \\mid \\texttt{i})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{<s>} , \\texttt{i}) + 1}{count(<s>) + |V|}) = \\log_{2} (\\frac{2006 + 1}{100000 + 41739}) = -6.142052348726812  \\\\\n",
            "\\log_{2} (P(\\texttt{i} \\mid \\texttt{look})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{i} , \\texttt{look}) + 1}{count(i) + |V|}) = \\log_{2} (\\frac{15 + 1}{7339 + 41739}) = -11.582788837823436  \\\\\n",
            "\\log_{2} (P(\\texttt{look} \\mid \\texttt{forward})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{look} , \\texttt{forward}) + 1}{count(look) + |V|}) = \\log_{2} (\\frac{34 + 1}{613 + 41739}) = -10.240859462550434  \\\\\n",
            "\\log_{2} (P(\\texttt{forward} \\mid \\texttt{to})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{forward} , \\texttt{to}) + 1}{count(forward) + |V|}) = \\log_{2} (\\frac{100 + 1}{474 + 41739}) = -8.707188259410588  \\\\\n",
            "\\log_{2} (P(\\texttt{to} \\mid \\texttt{hearing})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{to} , \\texttt{hearing}) + 1}{count(to) + |V|}) = \\log_{2} (\\frac{6 + 1}{53048 + 41739}) = -13.725046665121754  \\\\\n",
            "\\log_{2} (P(\\texttt{hearing} \\mid \\texttt{your})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{hearing} , \\texttt{your}) + 1}{count(hearing) + |V|}) = \\log_{2} (\\frac{0 + 1}{209 + 41739}) = -15.35631440692812  \\\\\n",
            "\\log_{2} (P(\\texttt{your} \\mid \\texttt{reply})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{your} , \\texttt{reply}) + 1}{count(your) + |V|}) = \\log_{2} (\\frac{0 + 1}{1217 + 41739}) = -15.390572037471506  \\\\\n",
            "\\log_{2} (P(\\texttt{reply} \\mid \\texttt{.})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{reply} , \\texttt{.}) + 1}{count(reply) + |V|}) = \\log_{2} (\\frac{0 + 1}{13 + 41739}) = -15.349557686620518  \\\\\n",
            "\\log_{2} (P(\\texttt{.} \\mid \\texttt{</s>})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{.} , \\texttt{</s>}) + 1}{count(.) + |V|}) = \\log_{2} (\\frac{82888 + 1}{87894 + 41739}) = -0.6451804614204727  \\\\\n",
            "\\log_{2} (P(S)) &= \\log_{2} (P(\\texttt{I look forward to hearing your reply .})) \\\\ =&\\ \\log_{2} (P(\\texttt{<s>}, \\texttt{i}, \\texttt{look}, \\texttt{forward}, \\texttt{to}, \\texttt{hearing}, \\texttt{your}, \\texttt{reply}, \\texttt{.}, \\texttt{</s>})) \\\\ =&\\ \\log_{2} (P(\\texttt{<s>} \\mid \\texttt{i})) + \\log_{2} (P(\\texttt{i} \\mid \\texttt{look})) + \\log_{2} (P(\\texttt{look} \\mid \\texttt{forward})) + \\log_{2} (P(\\texttt{forward} \\mid \\texttt{to})) + \\log_{2} (P(\\texttt{to} \\mid \\texttt{hearing})) + \\log_{2} (P(\\texttt{hearing} \\mid \\texttt{your})) + \\log_{2} (P(\\texttt{your} \\mid \\texttt{reply})) + \\log_{2} (P(\\texttt{reply} \\mid \\texttt{.})) + \\log_{2} (P(\\texttt{.} \\mid \\texttt{</s>})) \\\\ =&\\ -6.142052348726812 + -11.582788837823436 + -10.240859462550434 + -8.707188259410588 + -13.725046665121754 + -15.35631440692812 + -15.390572037471506 + -15.349557686620518 + -0.6451804614204727 \\\\ =&\\ 9 \\\\ =&\\ -97.13956016607362 \\\\\n",
            "\\end{split}\\end{equation}\n",
            "\\begin{equation}\\begin{split}\n",
            "S =&\\ \\texttt{I look forward to hearing your reply .} \\\\\n",
            "P(\\texttt{<s>} \\mid \\texttt{i}) =&\\  \\frac{count^{*}(\\texttt{<s>} , \\texttt{i}) + 1}{count(<s>) + |V|} = \\frac{2006 + 1}{100000 + 41739} = 0.014159828981437713  \\\\\n",
            "P(\\texttt{i} \\mid \\texttt{look}) =&\\  \\frac{count^{*}(\\texttt{i} , \\texttt{look}) + 1}{count(i) + |V|} = \\frac{15 + 1}{7339 + 41739} = 0.0003260116549166633  \\\\\n",
            "P(\\texttt{look} \\mid \\texttt{forward}) =&\\  \\frac{count^{*}(\\texttt{look} , \\texttt{forward}) + 1}{count(look) + |V|} = \\frac{34 + 1}{613 + 41739} = 0.0008264072534945221  \\\\\n",
            "P(\\texttt{forward} \\mid \\texttt{to}) =&\\  \\frac{count^{*}(\\texttt{forward} , \\texttt{to}) + 1}{count(forward) + |V|} = \\frac{100 + 1}{474 + 41739} = 0.002392627863454386  \\\\\n",
            "P(\\texttt{to} \\mid \\texttt{hearing}) =&\\  \\frac{count^{*}(\\texttt{to} , \\texttt{hearing}) + 1}{count(to) + |V|} = \\frac{6 + 1}{53048 + 41739} = 7.384978952809984e-05  \\\\\n",
            "P(\\texttt{hearing} \\mid \\texttt{your}) =&\\  \\frac{count^{*}(\\texttt{hearing} , \\texttt{your}) + 1}{count(hearing) + |V|} = \\frac{0 + 1}{209 + 41739} = 2.3839038809955182e-05  \\\\\n",
            "P(\\texttt{your} \\mid \\texttt{reply}) =&\\  \\frac{count^{*}(\\texttt{your} , \\texttt{reply}) + 1}{count(your) + |V|} = \\frac{0 + 1}{1217 + 41739} = 2.3279634975323588e-05  \\\\\n",
            "P(\\texttt{reply} \\mid \\texttt{.}) =&\\  \\frac{count^{*}(\\texttt{reply} , \\texttt{.}) + 1}{count(reply) + |V|} = \\frac{0 + 1}{13 + 41739} = 2.395094845755892e-05  \\\\\n",
            "P(\\texttt{.} \\mid \\texttt{</s>}) =&\\  \\frac{count^{*}(\\texttt{.} , \\texttt{</s>}) + 1}{count(.) + |V|} = \\frac{82888 + 1}{87894 + 41739} = 0.6394128038385287  \\\\\n",
            "P(S) &= P(\\texttt{I look forward to hearing your reply .}) \\\\ =&\\ P(\\texttt{<s>}, \\texttt{i}, \\texttt{look}, \\texttt{forward}, \\texttt{to}, \\texttt{hearing}, \\texttt{your}, \\texttt{reply}, \\texttt{.}, \\texttt{</s>}) \\\\ =&\\ P(\\texttt{<s>} \\mid \\texttt{i}) \\times P(\\texttt{i} \\mid \\texttt{look}) \\times P(\\texttt{look} \\mid \\texttt{forward}) \\times P(\\texttt{forward} \\mid \\texttt{to}) \\times P(\\texttt{to} \\mid \\texttt{hearing}) \\times P(\\texttt{hearing} \\mid \\texttt{your}) \\times P(\\texttt{your} \\mid \\texttt{reply}) \\times P(\\texttt{reply} \\mid \\texttt{.}) \\times P(\\texttt{.} \\mid \\texttt{</s>}) \\\\ =&\\ 0.014159828981437713 \\times 0.0003260116549166633 \\times 0.0008264072534945221 \\times 0.002392627863454386 \\times 7.384978952809984e-05 \\times 2.3839038809955182e-05 \\times 2.3279634975323588e-05 \\times 2.395094845755892e-05 \\times 0.6394128038385287 \\\\ =&\\ 9 \\\\ =&\\ 5.728997390142119e-30 \\\\\n",
            "\\end{split}\\end{equation}\n"
          ]
        }
      ],
      "source": [
        "smoothBigramModel = SmoothBigramModel(train_unigrams_with_replacing, train_bigrams_with_replacing, ignoreWords={})\n",
        "smoothBigramLogOutput = smoothBigramModel.sentenceMLE('I look forward to hearing your reply .', verbose=True, log=True)\n",
        "output = smoothBigramModel.sentenceMLE('I look forward to hearing your reply .', verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\begin{equation}\\begin{split}\n",
            "S =&\\ \\texttt{I look forward to hearing your reply .} \\\\\n",
            "\\log_{2} (P(\\texttt{<s>} \\mid \\texttt{i})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{<s>} , \\texttt{i})}{count(\\texttt{<s>})}) = \\log_{2} (\\frac{2005.5}{100000}) = -5.639894223622303  \\\\\n",
            "\\log_{2} (P(\\texttt{i} \\mid \\texttt{look})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{i} , \\texttt{look})}{count(\\texttt{i})}) = \\log_{2} (\\frac{14.5}{7339}) = -8.983386786754767  \\\\\n",
            "\\log_{2} (P(\\texttt{look} \\mid \\texttt{forward})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{look} , \\texttt{forward})}{count(\\texttt{look})}) = \\log_{2} (\\frac{33.5}{613}) = -4.193654073233009  \\\\\n",
            "\\log_{2} (P(\\texttt{forward} \\mid \\texttt{to})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{forward} , \\texttt{to})}{count(\\texttt{forward})}) = \\log_{2} (\\frac{99.5}{474}) = -2.2521186283546104  \\\\\n",
            "\\log_{2} (P(\\texttt{to} \\mid \\texttt{hearing})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{to} , \\texttt{hearing})}{count(\\texttt{to})}) = \\log_{2} (\\frac{5.5}{53048}) = -13.23557912101594  \\\\\n",
            "\\alpha_{\\texttt{hearing}} =&\\  1 - \\frac{\\Sigma_{w} count^{*}(\\texttt{hearing} , \\texttt{your})}{count(\\texttt{hearing})} = 1 - \\frac{170.0}{209} = 0.1866028708133971  \\\\\n",
            "\\log_{2} (P(\\texttt{hearing} \\mid \\texttt{your})) =&\\  \\log_{2} (\\alpha_{\\texttt{hearing}} \\times \\frac{P_{ML}(\\texttt{your})}{\\Sigma_{w \\in B_{\\texttt{hearing}}} P(\\texttt{w})}) = \\log_{2} (0.1866028708133971\\times \\frac{0.00047387090619536564}{0.6730450391519687}) = -12.893950161003882  \\\\\n",
            "\\alpha_{\\texttt{your}} =&\\  1 - \\frac{\\Sigma_{w} count^{*}(\\texttt{your} , \\texttt{reply})}{count(\\texttt{your})} = 1 - \\frac{874.5}{1217} = 0.2814297452752671  \\\\\n",
            "\\log_{2} (P(\\texttt{your} \\mid \\texttt{reply})) =&\\  \\log_{2} (\\alpha_{\\texttt{your}} \\times \\frac{P_{ML}(\\texttt{reply})}{\\Sigma_{w \\in B_{\\texttt{your}}} P(\\texttt{w})}) = \\log_{2} (0.2814297452752671\\times \\frac{5.061891356236445e-06}{0.8819099684217718}) = -19.239748589030043  \\\\\n",
            "\\alpha_{\\texttt{reply}} =&\\  1 - \\frac{\\Sigma_{w} count^{*}(\\texttt{reply} , \\texttt{.})}{count(\\texttt{reply})} = 1 - \\frac{10.0}{13} = 0.23076923076923073  \\\\\n",
            "\\log_{2} (P(\\texttt{reply} \\mid \\texttt{.})) =&\\  \\log_{2} (\\alpha_{\\texttt{reply}} \\times \\frac{P_{ML}(\\texttt{.})}{\\Sigma_{w \\in B_{\\texttt{reply}}} P(\\texttt{w})}) = \\log_{2} (0.23076923076923073\\times \\frac{0.03422383683577278}{0.9201778670749203}) = -6.864316558703887  \\\\\n",
            "\\log_{2} (P(\\texttt{.} \\mid \\texttt{</s>})) =&\\  \\log_{2} (\\frac{count^{*}(\\texttt{.} , \\texttt{</s>})}{count(\\texttt{.})}) = \\log_{2} (\\frac{82887.5}{87894}) = -0.08461013465181358  \\\\\n",
            "\\log_{2} (P(S)) &= \\log_{2} (P(\\texttt{I look forward to hearing your reply .})) \\\\ =&\\ \\log_{2} (P(\\texttt{<s>}, \\texttt{i}, \\texttt{look}, \\texttt{forward}, \\texttt{to}, \\texttt{hearing}, \\texttt{your}, \\texttt{reply}, \\texttt{.}, \\texttt{</s>})) \\\\ =&\\ \\log_{2} (P(\\texttt{<s>} \\mid \\texttt{i})) + \\log_{2} (P(\\texttt{i} \\mid \\texttt{look})) + \\log_{2} (P(\\texttt{look} \\mid \\texttt{forward})) + \\log_{2} (P(\\texttt{forward} \\mid \\texttt{to})) + \\log_{2} (P(\\texttt{to} \\mid \\texttt{hearing})) + \\log_{2} (P(\\texttt{hearing} \\mid \\texttt{your})) + \\log_{2} (P(\\texttt{your} \\mid \\texttt{reply})) + \\log_{2} (P(\\texttt{reply} \\mid \\texttt{.})) + \\log_{2} (P(\\texttt{.} \\mid \\texttt{</s>})) \\\\ =&\\ -5.639894223622303 + -8.983386786754767 + -4.193654073233009 + -2.2521186283546104 + -13.23557912101594 + -12.893950161003882 + -19.239748589030043 + -6.864316558703887 + -0.08461013465181358 \\\\ =&\\ 9 \\\\ =&\\ -73.38725827637026 \\\\\n",
            "\\end{split}\\end{equation}\n",
            "\\begin{equation}\\begin{split}\n",
            "S =&\\ \\texttt{I look forward to hearing your reply .} \\\\\n",
            "P(\\texttt{<s>} \\mid \\texttt{i}) =&\\  \\frac{count^{*}(\\texttt{<s>} , \\texttt{i})}{count(\\texttt{<s>})} = \\frac{2005.5}{100000} = 0.020055  \\\\\n",
            "P(\\texttt{i} \\mid \\texttt{look}) =&\\  \\frac{count^{*}(\\texttt{i} , \\texttt{look})}{count(\\texttt{i})} = \\frac{14.5}{7339} = 0.001975746014443385  \\\\\n",
            "P(\\texttt{look} \\mid \\texttt{forward}) =&\\  \\frac{count^{*}(\\texttt{look} , \\texttt{forward})}{count(\\texttt{look})} = \\frac{33.5}{613} = 0.05464926590538336  \\\\\n",
            "P(\\texttt{forward} \\mid \\texttt{to}) =&\\  \\frac{count^{*}(\\texttt{forward} , \\texttt{to})}{count(\\texttt{forward})} = \\frac{99.5}{474} = 0.20991561181434598  \\\\\n",
            "P(\\texttt{to} \\mid \\texttt{hearing}) =&\\  \\frac{count^{*}(\\texttt{to} , \\texttt{hearing})}{count(\\texttt{to})} = \\frac{5.5}{53048} = 0.00010367968632182174  \\\\\n",
            "\\alpha_{\\texttt{hearing}} =&\\  1 - \\frac{\\Sigma_{w} count^{*}(\\texttt{hearing} , \\texttt{your})}{count(\\texttt{hearing})} = 1 - \\frac{170.0}{209} = 0.1866028708133971  \\\\\n",
            "P(\\texttt{hearing} \\mid \\texttt{your}) =&\\  \\alpha_{\\texttt{hearing}} \\times \\frac{P_{ML}(\\texttt{your})}{\\Sigma_{w \\in B_{\\texttt{hearing}}} P(\\texttt{w})} = 0.1866028708133971\\times \\frac{0.00047387090619536564}{0.6730450391519687} = 0.00013138150695296243  \\\\\n",
            "\\alpha_{\\texttt{your}} =&\\  1 - \\frac{\\Sigma_{w} count^{*}(\\texttt{your} , \\texttt{reply})}{count(\\texttt{your})} = 1 - \\frac{874.5}{1217} = 0.2814297452752671  \\\\\n",
            "P(\\texttt{your} \\mid \\texttt{reply}) =&\\  \\alpha_{\\texttt{your}} \\times \\frac{P_{ML}(\\texttt{reply})}{\\Sigma_{w \\in B_{\\texttt{your}}} P(\\texttt{w})} = 0.2814297452752671\\times \\frac{5.061891356236445e-06}{0.8819099684217718} = 1.6153199827710787e-06  \\\\\n",
            "\\alpha_{\\texttt{reply}} =&\\  1 - \\frac{\\Sigma_{w} count^{*}(\\texttt{reply} , \\texttt{.})}{count(\\texttt{reply})} = 1 - \\frac{10.0}{13} = 0.23076923076923073  \\\\\n",
            "P(\\texttt{reply} \\mid \\texttt{.}) =&\\  \\alpha_{\\texttt{reply}} \\times \\frac{P_{ML}(\\texttt{.})}{\\Sigma_{w \\in B_{\\texttt{reply}}} P(\\texttt{w})} = 0.23076923076923073\\times \\frac{0.03422383683577278}{0.9201778670749203} = 0.00858291508974092  \\\\\n",
            "P(\\texttt{.} \\mid \\texttt{</s>}) =&\\  \\frac{count^{*}(\\texttt{.} , \\texttt{</s>})}{count(\\texttt{.})} = \\frac{82887.5}{87894} = 0.9430393428447903  \\\\\n",
            "P(S) &= P(\\texttt{I look forward to hearing your reply .}) \\\\ =&\\ P(\\texttt{<s>}, \\texttt{i}, \\texttt{look}, \\texttt{forward}, \\texttt{to}, \\texttt{hearing}, \\texttt{your}, \\texttt{reply}, \\texttt{.}, \\texttt{</s>}) \\\\ =&\\ P(\\texttt{<s>} \\mid \\texttt{i}) \\times P(\\texttt{i} \\mid \\texttt{look}) \\times P(\\texttt{look} \\mid \\texttt{forward}) \\times P(\\texttt{forward} \\mid \\texttt{to}) \\times P(\\texttt{to} \\mid \\texttt{hearing}) \\times P(\\texttt{hearing} \\mid \\texttt{your}) \\times P(\\texttt{your} \\mid \\texttt{reply}) \\times P(\\texttt{reply} \\mid \\texttt{.}) \\times P(\\texttt{.} \\mid \\texttt{</s>}) \\\\ =&\\ 0.020055 \\times 0.001975746014443385 \\times 0.05464926590538336 \\times 0.20991561181434598 \\times 0.00010367968632182174 \\times 0.00013138150695296243 \\times 1.6153199827710787e-06 \\times 0.00858291508974092 \\times 0.9430393428447903 \\\\ =&\\ 9 \\\\ =&\\ 8.095318855724841e-23 \\\\\n",
            "\\end{split}\\end{equation}\n"
          ]
        }
      ],
      "source": [
        "katzBigramModel = KatzBigramModel(train_unigrams_with_replacing, train_bigrams_with_replacing, ignoreWords={})\n",
        "katzBigramModel.trainBs(test_only_bigrams)\n",
        "katzBigramLogOutput = katzBigramModel.sentenceMLE('I look forward to hearing your reply .', verbose=True, log=True)\n",
        "katzBigramOutput = katzBigramModel.sentenceMLE('I look forward to hearing your reply .', verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. (20 points) Compute the perplexity of the sentence above under each of the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\paragraph{Unigram Model}\n",
            "\\begin{equation}\n",
            "\\begin{split}\n",
            "M_{S} &= 9 \\\\\n",
            "\\log_{2} (P_{ML}(S)) &= -89.74040857086109 \\\\\n",
            "l &= \\frac{\\log_{2} (P_{ML}(S))}{M_{S}} \\\\ &=\\ \\frac{-89.74040857086109}9 \\\\ &=\\ -9.971156507873454 \\\\\n",
            "Perplexity(S) &= 2^{-l} = 1003.7306831109403\\\\\n",
            "\\end{split}\n",
            "\\end{equation}\n",
            "\\paragraph{Bigram Model}\n",
            "\\begin{equation}\n",
            "\\begin{split}\n",
            "M_{S} &= 9 \\\\\n",
            "\\log_{2} (P_{ML}(S)) &= -inf \\\\\n",
            "l &= \\frac{\\log_{2} (P_{ML}(S))}{M_{S}} \\\\ &=\\ \\frac{-inf}9 \\\\ &=\\ -inf \\\\\n",
            "Perplexity(S) &= 2^{-l} = inf\\\\\n",
            "\\end{split}\n",
            "\\end{equation}\n",
            "\\paragraph{Smooth Bigram Model}\n",
            "\\begin{equation}\n",
            "\\begin{split}\n",
            "M_{S} &= 9 \\\\\n",
            "\\log_{2} (P_{ML}(S)) &= -97.13956016607362 \\\\\n",
            "l &= \\frac{\\log_{2} (P_{ML}(S))}{M_{S}} \\\\ &=\\ \\frac{-97.13956016607362}9 \\\\ &=\\ -10.793284462897068 \\\\\n",
            "Perplexity(S) &= 2^{-l} = 1774.607755085189\\\\\n",
            "\\end{split}\n",
            "\\end{equation}\n",
            "\\paragraph{Katz Bigram Model}\n",
            "\\begin{equation}\n",
            "\\begin{split}\n",
            "M_{S} &= 9 \\\\\n",
            "\\log_{2} (P_{ML}(S)) &= -73.38725827637026 \\\\\n",
            "l &= \\frac{\\log_{2} (P_{ML}(S))}{M_{S}} \\\\ &=\\ \\frac{-73.38725827637026}9 \\\\ &=\\ -8.154139808485585 \\\\\n",
            "Perplexity(S) &= 2^{-l} = 284.86603528933665\\\\\n",
            "\\end{split}\n",
            "\\end{equation}\n"
          ]
        }
      ],
      "source": [
        "for output, title in [\n",
        "    (unigramLogOutput, 'Unigram Model'),\n",
        "    (bigramLogOutput, 'Bigram Model'),\n",
        "    (smoothBigramLogOutput, 'Smooth Bigram Model'),\n",
        "    (katzBigramLogOutput, 'Katz Bigram Model')]:\n",
        "\n",
        "    *_, M, log_p = output\n",
        "    l = 1/M * log_p\n",
        "    pp = math.pow(2, -l)\n",
        "    print('\\\\paragraph{%s}' % title)\n",
        "    print('\\\\begin{equation}')\n",
        "    print('\\\\begin{split}')\n",
        "    print('M_{S} &= ' + str(M) + ' \\\\\\\\')\n",
        "    print('\\\\log_{2} (P_{ML}(S)) &= ' + str(log_p) + ' \\\\\\\\')\n",
        "    print('l &= \\\\frac{\\\\log_{2} (P_{ML}(S))}{M_{S}} \\\\\\\\ &=\\ \\\\frac{' + str(log_p) + '}' + str(M) + ' \\\\\\\\ &=\\ ' + str(l) +' \\\\\\\\')\n",
        "    print('Perplexity(S) &= 2^{-l} = ' + str(pp) + '\\\\\\\\')\n",
        "    print('\\\\end{split}')\n",
        "    print('\\\\end{equation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. (20 points) Compute the perplexity of the entire test corpus under each of the models.Discuss the differences in the results you obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "for sentence in open('test.txt').read().strip().split('\\n'):\n",
        "    # sentence = ' '.join(words[1:-1])\n",
        "    results.append([ model.sentenceMLE(sentence, log=True) for model in [unigramModel, bigramModel, smoothBigramModel, katzBigramModel] ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\paragraph{unigramModel}\n",
            "\\begin{equation}\n",
            "\\begin{split}\n",
            "M_{C} &= \\Sigma_{S \\in C} M_{S} = 2769 \\\\\n",
            "\\log_{2} (P_{ML}(C)) &= \\Sigma_{S \\in C} \\log_{2} (P_{ML}(S)) = -27965.787053030697 \\\\\n",
            "l_{C} &= \\frac{\\log_{2}(P_{ML}(C))}{M_{C}} \\\\ &=\\  \\frac{-27965.787053030697}2769 \\\\ &=\\   -10.099598068989057 \\\\\n",
            "Perplexity(C) &= 2^{-l_{C}} = 1097.190308743997\\\\\n",
            "\\end{split}\n",
            "\\end{equation}\n",
            "\\paragraph{bigramModel}\n",
            "\\begin{equation}\n",
            "\\begin{split}\n",
            "M_{C} &= \\Sigma_{S \\in C} M_{S} = 2769 \\\\\n",
            "\\log_{2} (P_{ML}(C)) &= \\Sigma_{S \\in C} \\log_{2} (P_{ML}(S)) = -inf \\\\\n",
            "l_{C} &= \\frac{\\log_{2}(P_{ML}(C))}{M_{C}} \\\\ &=\\  \\frac{-inf}2769 \\\\ &=\\   -inf \\\\\n",
            "Perplexity(C) &= 2^{-l_{C}} = inf\\\\\n",
            "\\end{split}\n",
            "\\end{equation}\n",
            "\\paragraph{smoothBigramModel}\n",
            "\\begin{equation}\n",
            "\\begin{split}\n",
            "M_{C} &= \\Sigma_{S \\in C} M_{S} = 2769 \\\\\n",
            "\\log_{2} (P_{ML}(C)) &= \\Sigma_{S \\in C} \\log_{2} (P_{ML}(S)) = -31072.441669718453 \\\\\n",
            "l_{C} &= \\frac{\\log_{2}(P_{ML}(C))}{M_{C}} \\\\ &=\\  \\frac{-31072.441669718453}2769 \\\\ &=\\   -11.221539064542599 \\\\\n",
            "Perplexity(C) &= 2^{-l_{C}} = 2387.9204561337933\\\\\n",
            "\\end{split}\n",
            "\\end{equation}\n",
            "\\paragraph{katzBigramModel}\n",
            "\\begin{equation}\n",
            "\\begin{split}\n",
            "M_{C} &= \\Sigma_{S \\in C} M_{S} = 2769 \\\\\n",
            "\\log_{2} (P_{ML}(C)) &= \\Sigma_{S \\in C} \\log_{2} (P_{ML}(S)) = -23158.85155443307 \\\\\n",
            "l_{C} &= \\frac{\\log_{2}(P_{ML}(C))}{M_{C}} \\\\ &=\\  \\frac{-23158.85155443307}2769 \\\\ &=\\   -8.363615584844013 \\\\\n",
            "Perplexity(C) &= 2^{-l_{C}} = 329.381469853553\\\\\n",
            "\\end{split}\n",
            "\\end{equation}\n"
          ]
        }
      ],
      "source": [
        "for index, title in enumerate(('unigramModel', 'bigramModel', 'smoothBigramModel', 'katzBigramModel')):\n",
        "    sumLogP = 0\n",
        "    sumM = 0\n",
        "    sumL = 0\n",
        "    for result in results:\n",
        "        *_, M, logP = result[index]\n",
        "        sumLogP += logP\n",
        "        sumM += M\n",
        "        sumL += logP/M\n",
        "    corpusL = 1/sumM * sumLogP\n",
        "    corpusPP = math.pow(2, -corpusL)\n",
        "\n",
        "    print('\\\\paragraph{%s}' % title)\n",
        "    print('\\\\begin{equation}')\n",
        "    print('\\\\begin{split}')\n",
        "    print('M_{C} &= \\Sigma_{S \\in C} M_{S} = ' + str(sumM) + ' \\\\\\\\')\n",
        "    print('\\\\log_{2} (P_{ML}(C)) &= \\Sigma_{S \\in C} \\\\log_{2} (P_{ML}(S)) = ' + str(sumLogP) + ' \\\\\\\\')\n",
        "    print('l_{C} &= \\\\frac{\\\\log_{2}(P_{ML}(C))}{M_{C}} \\\\\\\\ &=\\  \\\\frac{' + str(sumLogP) + '}' + str(sumM) + ' \\\\\\\\ &=\\   ' + str(corpusL) +' \\\\\\\\')\n",
        "    print('Perplexity(C) &= 2^{-l_{C}} = ' + str(corpusPP) + '\\\\\\\\')\n",
        "    print('\\\\end{split}')\n",
        "    print('\\\\end{equation}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "685 41054\n",
            "41739 41739\n",
            "len(P_unigram) 41739\n"
          ]
        }
      ],
      "source": [
        "# katzBigramModel2 = KatzBigramModel(train_unigrams_with_replacing, train_bigrams_with_replacing, ignoreWords={})\n",
        "# katzBigramModel2.trainBs(test_only_bigrams)\n",
        "\n",
        "A_your = {}\n",
        "for unigram in train_unigrams_with_replacing:\n",
        "    if unigram in train_bigrams_with_replacing['your']:\n",
        "        A_your[unigram] = train_bigrams_with_replacing['your'][unigram]\n",
        "B_your = {}\n",
        "for unigram, count in train_unigrams_with_replacing.items():\n",
        "    if unigram in A_your:\n",
        "        continue\n",
        "    B_your[unigram] = count\n",
        "print(len(A_your), len(B_your))\n",
        "print(len(A_your) + len(B_your), len(train_unigrams_with_replacing))\n",
        "\n",
        "P_unigram = { word: katzBigramModel2.unigramMLE(word)[-1] for word in train_unigrams_with_replacing }\n",
        "print('len(P_unigram)', len(P_unigram))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.1180900315784147 0.8819099684217718 1.0000000000001865\n"
          ]
        }
      ],
      "source": [
        "P_A_your = sum(P_unigram[word] for word in A_your)\n",
        "P_B_your = sum(P_unigram[word] for word in B_your)\n",
        "print(P_A_your, P_B_your, P_A_your + P_B_your)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "685 41054\n",
            "0.1180900315784147 0.8819099684217718 1.0000000000001865\n"
          ]
        }
      ],
      "source": [
        "P_A_your_model = sum(katzBigramModel2.P_ML[word] for word in katzBigramModel2.A['your'])\n",
        "P_B_your_model = sum(katzBigramModel2.P_ML[word] for word in katzBigramModel2.B['your'])\n",
        "print(len(katzBigramModel2.A['your']), len(katzBigramModel2.B['your']))\n",
        "print(P_A_your_model, P_B_your_model, P_A_your_model + P_B_your_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1248"
            ]
          },
          "execution_count": 326,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from functools import reduce\n",
        "\n",
        "bigrams2 = [ [(condition, word) in words] for condition, words in test_bigrams.items() ]\n",
        "bigrams3 = reduce(lambda x, y: x + y, bigrams2, [])\n",
        "len(bigrams3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6282788401261891"
            ]
          },
          "execution_count": 264,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "P_B_your"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPu+5nqDTetxfSaS64LcZBi",
      "include_colab_link": true,
      "name": "part-2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
